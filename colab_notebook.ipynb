{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmolLMv3 + TRM Training\n",
    "\n",
    "Train SmolLMv3-3B with Tiny Recursive Model (TRM) for enhanced reasoning.\n",
    "\n",
    "**Architecture:**\n",
    "- SmolLMv3-3B with LoRA adapters\n",
    "- Perceiver-style latent attention compression (256x)\n",
    "- TRM recursive reasoning (2 layers, effective depth 672)\n",
    "- Sliding window output\n",
    "\n",
    "**Based on:**\n",
    "- [Less is More: Recursive Reasoning with Tiny Networks](https://arxiv.org/abs/2505.00000) by Alexia Jolicoeur-Martineau\n",
    "- [SmolLM3 Blog](https://huggingface.co/blog/smollm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")\n",
    "\n",
    "# Install dependencies if on Colab\n",
    "if IN_COLAB:\n",
    "    !pip install -q torch>=2.0.0\n",
    "    !pip install -q transformers>=4.30.0\n",
    "    !pip install -q peft>=0.4.0\n",
    "    !pip install -q pytorch-lightning>=2.0.0\n",
    "    !pip install -q wandb>=0.15.0\n",
    "    !pip install -q datasets>=2.14.0\n",
    "    print(\"\\nDependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify setup\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"Device: Apple Silicon (MPS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "import json\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TRM Core Components\n",
    "\n",
    "From `src/models/trm.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Standard transformer block with self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_heads: int = 8, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.RMSNorm(d_model)\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            d_model, n_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.norm2 = nn.RMSNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x, _ = self.self_attn(x, x, x)\n",
    "        x = x + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + residual\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TinyRecursiveNetwork(nn.Module):\n",
    "    \"\"\"The core tiny network for recursive reasoning (2 layers optimal)\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_layers: int = 2,\n",
    "        n_heads: int = 8,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RecursiveReasoningBase(nn.Module):\n",
    "    \"\"\"Base class with core recursion logic from TRM paper\"\"\"\n",
    "\n",
    "    def latent_recursion(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        z: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Update z n times, then update y once\"\"\"\n",
    "        for _ in range(self.n_latent_steps):\n",
    "            combined = x + y + z\n",
    "            z = self.net(combined)\n",
    "\n",
    "        combined = y + z\n",
    "        y = self.net(combined)\n",
    "\n",
    "        return y, z\n",
    "\n",
    "    def run_deep_recursion(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        z: torch.Tensor,\n",
    "        with_gradients: bool = True\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"T-1 recursions without gradients, 1 with gradients\"\"\"\n",
    "        if self.n_deep_recursions > 1:\n",
    "            with torch.no_grad():\n",
    "                for _ in range(self.n_deep_recursions - 1):\n",
    "                    y, z = self.latent_recursion(x, y, z)\n",
    "\n",
    "        if with_gradients:\n",
    "            y, z = self.latent_recursion(x, y, z)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                y, z = self.latent_recursion(x, y, z)\n",
    "\n",
    "        return y, z\n",
    "\n",
    "    def compute_halt_probability(self, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"ACT halting mechanism\"\"\"\n",
    "        halt_logits = self.halt_head(y.mean(dim=1))\n",
    "        return torch.sigmoid(halt_logits)\n",
    "\n",
    "print(\"TRM core components defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Latent Attention Compressor\n",
    "\n",
    "From `src/models/compression.py` - Perceiver-style compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentAttentionCompressor(nn.Module):\n",
    "    \"\"\"Perceiver-style compression: [B, L, D] -> [B, M, D]\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_latents: int,\n",
    "        n_heads: int = 8,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_latents = num_latents\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Learned latent queries\n",
    "        self.latent_queries = nn.Parameter(torch.randn(num_latents, hidden_size))\n",
    "\n",
    "        # Cross-attention\n",
    "        self.compress_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.compress_norm = nn.LayerNorm(hidden_size)\n",
    "        self.compress_ff = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size * 4, hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.compress_ff_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        latents = self.latent_queries.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "        key_padding_mask = None\n",
    "        if attention_mask is not None:\n",
    "            key_padding_mask = (attention_mask == 0)\n",
    "\n",
    "        attn_out, _ = self.compress_attn(\n",
    "            query=latents,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            key_padding_mask=key_padding_mask\n",
    "        )\n",
    "        latents = self.compress_norm(latents + attn_out)\n",
    "\n",
    "        ff_out = self.compress_ff(latents)\n",
    "        latents = self.compress_ff_norm(latents + ff_out)\n",
    "\n",
    "        return latents\n",
    "\n",
    "print(\"Latent Attention Compressor defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hidden State TRM\n",
    "\n",
    "From `src/models/smollm.py` - TRM for LLM hidden states with sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenStateTRM(RecursiveReasoningBase):\n",
    "    \"\"\"TRM for LLM hidden states with sliding window output\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 3072,\n",
    "        num_latents: int = 256,\n",
    "        n_layers: int = 2,\n",
    "        n_heads: int = 8,\n",
    "        compression_heads: int = 8,\n",
    "        n_latent_steps: int = 6,\n",
    "        n_deep_recursions: int = 3,\n",
    "        n_supervision_steps: int = 8,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_latents = num_latents\n",
    "        self.n_latent_steps = n_latent_steps\n",
    "        self.n_deep_recursions = n_deep_recursions\n",
    "        self.n_supervision_steps = n_supervision_steps\n",
    "\n",
    "        self.compressor = LatentAttentionCompressor(\n",
    "            hidden_size=hidden_size,\n",
    "            num_latents=num_latents,\n",
    "            n_heads=compression_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.net = TinyRecursiveNetwork(\n",
    "            d_model=hidden_size,\n",
    "            n_layers=n_layers,\n",
    "            n_heads=n_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.halt_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        return_all_steps: bool = False\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = hidden_states.shape\n",
    "\n",
    "        # Compress: [B, L, D] -> [B, M, D]\n",
    "        x_compressed = self.compressor(hidden_states, attention_mask=attention_mask)\n",
    "\n",
    "        y = torch.zeros_like(x_compressed)\n",
    "        z = torch.zeros_like(x_compressed)\n",
    "\n",
    "        all_outputs = []\n",
    "\n",
    "        for step in range(self.n_supervision_steps):\n",
    "            y, z = self.run_deep_recursion(x_compressed, y, z, with_gradients=True)\n",
    "\n",
    "            if return_all_steps:\n",
    "                shifted = torch.cat([\n",
    "                    hidden_states[:, self.num_latents:, :],\n",
    "                    y\n",
    "                ], dim=1)\n",
    "                all_outputs.append(shifted)\n",
    "\n",
    "            if not self.training:\n",
    "                halt_prob = self.compute_halt_probability(y)\n",
    "                if halt_prob.mean() > 0.5:\n",
    "                    break\n",
    "\n",
    "            y = y.detach()\n",
    "            z = z.detach()\n",
    "\n",
    "        # Sliding window: drop first M, append M TRM states\n",
    "        shifted_states = torch.cat([\n",
    "            hidden_states[:, self.num_latents:, :],\n",
    "            y\n",
    "        ], dim=1)\n",
    "\n",
    "        if return_all_steps:\n",
    "            return all_outputs\n",
    "        return shifted_states\n",
    "\n",
    "print(\"Hidden State TRM defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SmolLMv3 + TRM Integration\n",
    "\n",
    "Full integration with LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmolLMv3WithTRM(nn.Module):\n",
    "    \"\"\"SmolLMv3 with TRM for enhanced reasoning\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"HuggingFaceTB/SmolLM3-3B\",\n",
    "        use_lora: bool = True,\n",
    "        lora_r: int = 16,\n",
    "        lora_alpha: int = 32,\n",
    "        lora_dropout: float = 0.1,\n",
    "        num_latents: int = 256,\n",
    "        trm_kwargs: Optional[dict] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        if use_lora:\n",
    "            lora_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                r=lora_r,\n",
    "                lora_alpha=lora_alpha,\n",
    "                lora_dropout=lora_dropout,\n",
    "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "                bias=\"none\"\n",
    "            )\n",
    "            self.base_model = get_peft_model(self.base_model, lora_config)\n",
    "            print(\"\\nLoRA adapters applied:\")\n",
    "            self.base_model.print_trainable_parameters()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        special_tokens = {\"additional_special_tokens\": [\"<think>\"]}\n",
    "        num_added = self.tokenizer.add_special_tokens(special_tokens)\n",
    "        if num_added > 0:\n",
    "            self.base_model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        self.think_token_id = self.tokenizer.convert_tokens_to_ids(\"<think>\")\n",
    "\n",
    "        config = self.base_model.config\n",
    "        hidden_size = config.hidden_size\n",
    "\n",
    "        trm_kwargs = trm_kwargs or {}\n",
    "        print(f\"\\nInitializing TRM with {num_latents} latents...\")\n",
    "        self.trm = HiddenStateTRM(\n",
    "            hidden_size=hidden_size,\n",
    "            num_latents=num_latents,\n",
    "            **trm_kwargs\n",
    "        )\n",
    "\n",
    "        if not use_lora:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        print(f\"\\nModel initialized\")\n",
    "        print(f\"  <think> token ID: {self.think_token_id}\")\n",
    "        print(f\"  TRM parameters: {sum(p.numel() for p in self.trm.parameters())/1e6:.2f}M\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_trm: bool = True\n",
    "    ):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        if not use_trm or not self.training:\n",
    "            return outputs\n",
    "\n",
    "        think_positions = (input_ids == self.think_token_id).nonzero(as_tuple=True)\n",
    "\n",
    "        if len(think_positions[0]) == 0:\n",
    "            return outputs\n",
    "\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        shifted_states = self.trm(hidden_states, attention_mask=attention_mask)\n",
    "\n",
    "        trm_logits = self.base_model.lm_head(shifted_states)\n",
    "\n",
    "        if labels is not None:\n",
    "            shifted_labels = labels[:, self.trm.num_latents:]\n",
    "            if trm_logits.size(1) != shifted_labels.size(1):\n",
    "                trm_logits = trm_logits[:, :shifted_labels.size(1), :]\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            trm_loss = loss_fct(\n",
    "                trm_logits.reshape(-1, trm_logits.size(-1)),\n",
    "                shifted_labels.reshape(-1)\n",
    "            )\n",
    "            outputs.loss = outputs.loss + 0.3 * trm_loss\n",
    "\n",
    "        return outputs\n",
    "\n",
    "print(\"SmolLMv3WithTRM defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset and Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReasoningDataset(Dataset):\n",
    "    \"\"\"Dataset with <think> token for TRM reasoning\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: List[Dict],\n",
    "        tokenizer,\n",
    "        max_length: int = 512,\n",
    "        add_think_token: bool = True\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.add_think_token = add_think_token\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.data[idx]\n",
    "\n",
    "        question = item['question']\n",
    "        answer = item['answer']\n",
    "\n",
    "        if self.add_think_token:\n",
    "            text = f\"Question: {question}\\nAnswer: <think> {answer}\"\n",
    "        else:\n",
    "            text = f\"Question: {question}\\nAnswer: {answer}\"\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "\n",
    "def create_sample_dataset() -> List[Dict]:\n",
    "    \"\"\"Create sample math problems for testing\"\"\"\n",
    "    return [\n",
    "        {\"question\": \"What is 15 x 23?\", \"answer\": \"345\"},\n",
    "        {\"question\": \"What is 48 + 76?\", \"answer\": \"124\"},\n",
    "        {\"question\": \"What is 100 - 37?\", \"answer\": \"63\"},\n",
    "        {\"question\": \"What is 12 x 12?\", \"answer\": \"144\"},\n",
    "        {\"question\": \"What is 256 / 8?\", \"answer\": \"32\"},\n",
    "    ]\n",
    "\n",
    "print(\"Dataset utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PyTorch Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmolLMTRMLightningModule(pl.LightningModule):\n",
    "    \"\"\"PyTorch Lightning module for training\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"HuggingFaceTB/SmolLM3-3B\",\n",
    "        use_lora: bool = True,\n",
    "        lora_r: int = 16,\n",
    "        lora_alpha: int = 32,\n",
    "        num_latents: int = 256,\n",
    "        learning_rate: float = 2e-4,\n",
    "        weight_decay: float = 0.01,\n",
    "        warmup_steps: int = 100,\n",
    "        trm_kwargs: Optional[Dict] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = SmolLMv3WithTRM(\n",
    "            model_name=model_name,\n",
    "            use_lora=use_lora,\n",
    "            lora_r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            num_latents=num_latents,\n",
    "            trm_kwargs=trm_kwargs or {}\n",
    "        )\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        return self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            use_trm=True\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"]\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train/perplexity\", torch.exp(loss), on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"]\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/perplexity\", torch.exp(loss), on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "        total_steps = self.trainer.estimated_stepping_batches\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"Lightning Module defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Configuration\n",
    "\n",
    "Configure and run training (update paths as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    \"model_name\": \"HuggingFaceTB/SmolLM3-3B\",\n",
    "    \"batch_size\": 2,\n",
    "    \"num_epochs\": 3,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_latents\": 256,\n",
    "    \"accumulate_grad_batches\": 4,\n",
    "    \"precision\": \"bf16-mixed\",\n",
    "    \"output_dir\": \"./checkpoints\",\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data (replace with your dataset)\n",
    "sample_data = create_sample_dataset()\n",
    "print(f\"Sample data: {len(sample_data)} examples\")\n",
    "print(f\"Example: {sample_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run training\n",
    "# import wandb\n",
    "# wandb.login()\n",
    "#\n",
    "# pl_module = SmolLMTRMLightningModule(\n",
    "#     model_name=config[\"model_name\"],\n",
    "#     num_latents=config[\"num_latents\"],\n",
    "#     learning_rate=config[\"learning_rate\"],\n",
    "#     trm_kwargs={\n",
    "#         \"n_layers\": 2,\n",
    "#         \"n_latent_steps\": 6,\n",
    "#         \"n_deep_recursions\": 3,\n",
    "#         \"n_supervision_steps\": 8,\n",
    "#     }\n",
    "# )\n",
    "#\n",
    "# train_dataset = ReasoningDataset(sample_data, pl_module.model.tokenizer)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "#\n",
    "# trainer = pl.Trainer(\n",
    "#     max_epochs=config[\"num_epochs\"],\n",
    "#     accelerator=\"auto\",\n",
    "#     precision=config[\"precision\"],\n",
    "#     accumulate_grad_batches=config[\"accumulate_grad_batches\"],\n",
    "#     gradient_clip_val=1.0,\n",
    "# )\n",
    "#\n",
    "# trainer.fit(pl_module, train_dataloaders=train_loader)\n",
    "\n",
    "print(\"Training code ready - uncomment to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Notes\n",
    "\n",
    "### Training Phases (see src/train/)\n",
    "\n",
    "1. **Phase 1**: Compressor pretraining (identity + CoT)\n",
    "2. **Phase 2**: TRM iteration training (hidden_pre -> hidden_post)\n",
    "3. **Phase 3**: GRPO training (freeze LLM, train TRM + compressor)\n",
    "\n",
    "### Key Hyperparameters (from TRM paper)\n",
    "\n",
    "- `n_layers=2` (more layers â†’ overfitting)\n",
    "- `n_latent_steps=6` (n in paper)\n",
    "- `n_deep_recursions=3` (T in paper)\n",
    "- `n_supervision_steps=16` (N_sup in paper)\n",
    "- `ema_decay=0.999` (critical for stability)\n",
    "\n",
    "### References\n",
    "\n",
    "- TRM Paper: papers/less-is-more-TRM/paper.tex\n",
    "- SmolLM3: https://huggingface.co/blog/smollm3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
