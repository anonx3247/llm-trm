{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SmolLMv3 + TRM Training on GSM8K\n",
        "\n",
        "This notebook trains SmolLMv3 with Tiny Recursive Model (TRM) on the GSM8K (Grade School Math) dataset.\n",
        "\n",
        "**Dataset Split:**\n",
        "- **Training**: 80% of GSM8K train set (~5,978 examples)\n",
        "- **Test**: 20% of GSM8K train set (~1,495 examples)\n",
        "- **Final Evaluation**: Original GSM8K test set (1,319 examples) - kept separate\n",
        "\n",
        "**Features:**\n",
        "- Self-contained - no external imports needed\n",
        "- Proper train/test separation\n",
        "- PyTorch Lightning for clean training\n",
        "- Weights & Biases logging\n",
        "- LoRA for efficient fine-tuning\n",
        "- Latent attention compression\n",
        "- Automatic checkpointing\n",
        "\n",
        "**Based on:** *Less is More: Recursive Reasoning with Tiny Networks* by Alexia Jolicoeur-Martineau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running locally\n"
          ]
        }
      ],
      "source": [
        "# Check if running on Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running on Google Colab\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running locally\")\n",
        "\n",
        "# Install dependencies if on Colab\n",
        "if IN_COLAB:\n",
        "    !pip install -q torch>=2.0.0\n",
        "    !pip install -q transformers>=4.30.0\n",
        "    !pip install -q peft>=0.4.0\n",
        "    !pip install -q pytorch-lightning>=2.0.0\n",
        "    !pip install -q wandb>=0.15.0\n",
        "    !pip install -q datasets>=2.14.0\n",
        "    !pip install -q tqdm\n",
        "    print(\"\\nDependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.8.0\n",
            "CUDA available: False\n",
            "MPS available: True\n",
            "Device: Apple Silicon (MPS) - GPU acceleration enabled\n"
          ]
        }
      ],
      "source": [
        "# Verify PyTorch and GPU availability\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    print(\"Device: Apple Silicon (MPS) - GPU acceleration enabled\")\n",
        "else:\n",
        "    print(\"No GPU available - training will be slow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Login to Weights & Biases\n",
        "\n",
        "Get your API key from: https://wandb.ai/authorize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/neosapien/Development/llm-trm/.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/Users/neosapien/Development/llm-trm/.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mneosapien17\u001b[0m (\u001b[33manonx\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Import Core Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/neosapien/Development/llm-trm/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports successful\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import Tuple, Dict, List, Optional\n",
        "import json\n",
        "import re\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load and Split GSM8K Dataset\n",
        "\n",
        "**Split Strategy:**\n",
        "- GSM8K training set (7,473 examples) → 80% train / 20% test\n",
        "- GSM8K test set (1,319 examples) → Reserved for final evaluation only\n",
        "\n",
        "This gives us:\n",
        "- **Train**: ~5,978 examples (80% of 7,473)\n",
        "- **Test**: ~1,495 examples (20% of 7,473)\n",
        "- **Final Eval**: 1,319 examples (original test set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GSM8K dataset...\n",
            "Full training set: 7473 examples\n",
            "Final test set: 1319 examples (reserved for final evaluation)\n",
            "\n",
            "Creating 80/20 split:\n",
            "  Train: 5979 examples (80%)\n",
            "  Test: 1494 examples (20%)\n",
            "\n",
            "Final splits:\n",
            "  Train: 5979\n",
            "  Test: 1494\n",
            "  Final Eval: 1319\n",
            "\n",
            "Sample question:\n",
            "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
            "\n",
            "Sample answer:\n",
            "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
            "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
            "#### 72\n"
          ]
        }
      ],
      "source": [
        "# Load GSM8K dataset from HuggingFace\n",
        "print(\"Loading GSM8K dataset...\")\n",
        "gsm8k_full = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
        "gsm8k_final_test = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
        "\n",
        "print(f\"Full training set: {len(gsm8k_full)} examples\")\n",
        "print(f\"Final test set: {len(gsm8k_final_test)} examples (reserved for final evaluation)\")\n",
        "\n",
        "# Split the training set into train (80%) and test (20%)\n",
        "from datasets import Dataset as HFDataset\n",
        "\n",
        "# Calculate split index\n",
        "total_examples = len(gsm8k_full)\n",
        "test_size = int(0.2 * total_examples)\n",
        "train_size = total_examples - test_size\n",
        "\n",
        "print(f\"\\nCreating 80/20 split:\")\n",
        "print(f\"  Train: {train_size} examples (80%)\")\n",
        "print(f\"  Test: {test_size} examples (20%)\")\n",
        "\n",
        "# Create the split\n",
        "gsm8k_train = gsm8k_full.select(range(train_size))\n",
        "gsm8k_test = gsm8k_full.select(range(train_size, total_examples))\n",
        "\n",
        "print(f\"\\nFinal splits:\")\n",
        "print(f\"  Train: {len(gsm8k_train)}\")\n",
        "print(f\"  Test: {len(gsm8k_test)}\")\n",
        "print(f\"  Final Eval: {len(gsm8k_final_test)}\")\n",
        "\n",
        "# Show a sample\n",
        "sample = gsm8k_train[0]\n",
        "print(\"\\nSample question:\")\n",
        "print(sample['question'])\n",
        "print(\"\\nSample answer:\")\n",
        "print(sample['answer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. TRM Core Components\n",
        "\n",
        "### 5.1 Transformer Block and Tiny Recursive Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformer and Tiny Recursive Network defined\n"
          ]
        }
      ],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Standard transformer block with self-attention\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, n_heads: int = 8, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.norm1 = nn.RMSNorm(d_model)\n",
        "        self.self_attn = nn.MultiheadAttention(\n",
        "            d_model, n_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        \n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_model * 4, d_model)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Self-attention with residual\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x, _ = self.self_attn(x, x, x)\n",
        "        x = x + residual\n",
        "        \n",
        "        # MLP with residual\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = x + residual\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class TinyRecursiveNetwork(nn.Module):\n",
        "    \"\"\"The core tiny network for recursive reasoning\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_layers: int = 2,\n",
        "        n_heads: int = 8,\n",
        "        dropout: float = 0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(d_model, n_heads, dropout) \n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "print(\"Transformer and Tiny Recursive Network defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Latent Attention Compressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latent Attention Compressor defined\n"
          ]
        }
      ],
      "source": [
        "class LatentAttentionCompressor(nn.Module):\n",
        "    \"\"\"Attention-based sequence compression (Perceiver-style)\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        num_latents: int,\n",
        "        n_heads: int = 8,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_latents = num_latents\n",
        "        self.n_heads = n_heads\n",
        "        \n",
        "        # Learned latent queries\n",
        "        self.latent_queries = nn.Parameter(torch.randn(num_latents, hidden_size))\n",
        "        \n",
        "        # Cross-attention\n",
        "        self.compress_attn = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_size,\n",
        "            num_heads=n_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.compress_norm = nn.LayerNorm(hidden_size)\n",
        "        self.compress_ff = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size * 4, hidden_size),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.compress_ff_norm = nn.LayerNorm(hidden_size)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # Expand latent queries\n",
        "        latents = self.latent_queries.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        \n",
        "        # Cross-attention\n",
        "        # Convert attention_mask to key_padding_mask format (True for positions to ignore)\n",
        "        key_padding_mask = None\n",
        "\n",
        "        attention_mask = attention_mask.bool()\n",
        "        if attention_mask is not None:\n",
        "            key_padding_mask = (attention_mask == 0)  # True for padding positions\n",
        "        \n",
        "        attn_out, _ = self.compress_attn(\n",
        "            query=latents,\n",
        "            key=x,\n",
        "            value=x,\n",
        "            key_padding_mask=key_padding_mask\n",
        "        )\n",
        "        latents = self.compress_norm(latents + attn_out)\n",
        "        \n",
        "        # Feed-forward\n",
        "        ff_out = self.compress_ff(latents)\n",
        "        latents = self.compress_ff_norm(latents + ff_out)\n",
        "        \n",
        "        return latents\n",
        "\n",
        "print(\"Latent Attention Compressor defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Recursive Reasoning Base Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recursive Reasoning Base defined\n"
          ]
        }
      ],
      "source": [
        "class RecursiveReasoningBase(nn.Module):\n",
        "    \"\"\"Base class with core recursion logic\"\"\"\n",
        "    \n",
        "    def latent_recursion(\n",
        "        self, \n",
        "        x: torch.Tensor, \n",
        "        y: torch.Tensor, \n",
        "        z: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Update latent z\n",
        "        for _ in range(self.n_latent_steps):\n",
        "            combined = x + y + z\n",
        "            z = self.net(combined)\n",
        "        \n",
        "        # Update prediction y\n",
        "        combined = y + z\n",
        "        y = self.net(combined)\n",
        "        \n",
        "        return y, z\n",
        "    \n",
        "    def run_deep_recursion(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        y: torch.Tensor,\n",
        "        z: torch.Tensor,\n",
        "        with_gradients: bool = True\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Run T-1 recursions without gradients\n",
        "        if self.n_deep_recursions > 1:\n",
        "            with torch.no_grad():\n",
        "                for _ in range(self.n_deep_recursions - 1):\n",
        "                    y, z = self.latent_recursion(x, y, z)\n",
        "        \n",
        "        # Final recursion with gradients\n",
        "        if with_gradients:\n",
        "            y, z = self.latent_recursion(x, y, z)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                y, z = self.latent_recursion(x, y, z)\n",
        "        \n",
        "        return y, z\n",
        "    \n",
        "    def compute_halt_probability(self, y: torch.Tensor) -> torch.Tensor:\n",
        "        halt_logits = self.halt_head(y.mean(dim=1))\n",
        "        return torch.sigmoid(halt_logits)\n",
        "\n",
        "print(\"Recursive Reasoning Base defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Hidden State TRM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hidden State TRM defined\n"
          ]
        }
      ],
      "source": [
        "class HiddenStateTRM(RecursiveReasoningBase):\n",
        "    \"\"\"TRM for processing LLM hidden states with sliding window\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int = 3072,\n",
        "        num_latents: int = 256,\n",
        "        n_layers: int = 2,\n",
        "        n_heads: int = 8,\n",
        "        compression_heads: int = 8,\n",
        "        n_latent_steps: int = 6,\n",
        "        n_deep_recursions: int = 3,\n",
        "        n_supervision_steps: int = 8,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_latents = num_latents\n",
        "        self.n_latent_steps = n_latent_steps\n",
        "        self.n_deep_recursions = n_deep_recursions\n",
        "        self.n_supervision_steps = n_supervision_steps\n",
        "        \n",
        "        self.compressor = LatentAttentionCompressor(\n",
        "            hidden_size=hidden_size,\n",
        "            num_latents=num_latents,\n",
        "            n_heads=compression_heads,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        self.net = TinyRecursiveNetwork(\n",
        "            d_model=hidden_size,\n",
        "            n_layers=n_layers,\n",
        "            n_heads=n_heads,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        self.halt_head = nn.Linear(hidden_size, 1)\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        return_all_steps: bool = False\n",
        "    ) -> torch.Tensor:\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "        \n",
        "        # Compress\n",
        "        x_compressed = self.compressor(hidden_states, attention_mask=attention_mask)\n",
        "        \n",
        "        # Initialize\n",
        "        y = torch.zeros_like(x_compressed)\n",
        "        z = torch.zeros_like(x_compressed)\n",
        "        \n",
        "        all_outputs = []\n",
        "        \n",
        "        # Deep supervision loop\n",
        "        for step in range(self.n_supervision_steps):\n",
        "            y, z = self.run_deep_recursion(x_compressed, y, z, with_gradients=True)\n",
        "            \n",
        "            if return_all_steps:\n",
        "                shifted = torch.cat([\n",
        "                    hidden_states[:, self.num_latents:, :],\n",
        "                    y\n",
        "                ], dim=1)\n",
        "                all_outputs.append(shifted)\n",
        "            \n",
        "            if not self.training:\n",
        "                halt_prob = self.compute_halt_probability(y)\n",
        "                if halt_prob.mean() > 0.5:\n",
        "                    break\n",
        "            \n",
        "            y = y.detach()\n",
        "            z = z.detach()\n",
        "        \n",
        "        # Sliding window\n",
        "        shifted_states = torch.cat([\n",
        "            hidden_states[:, self.num_latents:, :],\n",
        "            y\n",
        "        ], dim=1)\n",
        "        \n",
        "        if return_all_steps:\n",
        "            return all_outputs\n",
        "        return shifted_states\n",
        "\n",
        "print(\"Hidden State TRM defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. SmolLMv3 + TRM Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SmolLMv3WithTRM defined\n"
          ]
        }
      ],
      "source": [
        "class SmolLMv3WithTRM(nn.Module):\n",
        "    \"\"\"SmolLMv3 with TRM for enhanced reasoning\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"HuggingFaceTB/SmolLM3-3B\",\n",
        "        use_lora: bool = True,\n",
        "        lora_r: int = 16,\n",
        "        lora_alpha: int = 32,\n",
        "        lora_dropout: float = 0.1,\n",
        "        num_latents: int = 256,\n",
        "        trm_kwargs: Optional[dict] = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        print(f\"Loading {model_name}...\")\n",
        "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        \n",
        "        if use_lora:\n",
        "            lora_config = LoraConfig(\n",
        "                task_type=TaskType.CAUSAL_LM,\n",
        "                r=lora_r,\n",
        "                lora_alpha=lora_alpha,\n",
        "                lora_dropout=lora_dropout,\n",
        "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "                bias=\"none\"\n",
        "            )\n",
        "            self.base_model = get_peft_model(self.base_model, lora_config)\n",
        "            print(\"\\nLoRA adapters applied:\")\n",
        "            self.base_model.print_trainable_parameters()\n",
        "        \n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        \n",
        "        # Add <think> token\n",
        "        special_tokens = {\"additional_special_tokens\": [\"<think>\"]}\n",
        "        num_added = self.tokenizer.add_special_tokens(special_tokens)\n",
        "        if num_added > 0:\n",
        "            self.base_model.resize_token_embeddings(len(self.tokenizer))\n",
        "        \n",
        "        self.think_token_id = self.tokenizer.convert_tokens_to_ids(\"<think>\")\n",
        "        \n",
        "        config = self.base_model.config\n",
        "        hidden_size = config.hidden_size\n",
        "        \n",
        "        trm_kwargs = trm_kwargs or {}\n",
        "        print(f\"\\nInitializing TRM with {num_latents} latents...\")\n",
        "        self.trm = HiddenStateTRM(\n",
        "            hidden_size=hidden_size,\n",
        "            num_latents=num_latents,\n",
        "            **trm_kwargs\n",
        "        )\n",
        "        \n",
        "        if not use_lora:\n",
        "            for param in self.base_model.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "        print(f\"\\nModel initialized\")\n",
        "        print(f\"  <think> token ID: {self.think_token_id}\")\n",
        "        print(f\"  TRM parameters: {sum(p.numel() for p in self.trm.parameters())/1e6:.2f}M\")\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        use_trm: bool = True\n",
        "    ):\n",
        "        outputs = self.base_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "        \n",
        "        if not use_trm or not self.training:\n",
        "            return outputs\n",
        "        \n",
        "        think_positions = (input_ids == self.think_token_id).nonzero(as_tuple=True)\n",
        "        \n",
        "        if len(think_positions[0]) == 0:\n",
        "            return outputs\n",
        "        \n",
        "        hidden_states = outputs.hidden_states[-1]\n",
        "        shifted_states = self.trm(hidden_states, attention_mask=attention_mask)\n",
        "        \n",
        "        trm_logits = self.base_model.lm_head(shifted_states)\n",
        "        \n",
        "        if labels is not None:\n",
        "            # Ensure dimensions match: TRM logits should match the shifted labels\n",
        "            shifted_labels = labels[:, self.trm.num_latents:]\n",
        "            # Make sure TRM logits and shifted labels have the same sequence length\n",
        "            if trm_logits.size(1) != shifted_labels.size(1):\n",
        "                # If dimensions don't match, adjust TRM logits to match shifted labels\n",
        "                trm_logits = trm_logits[:, :shifted_labels.size(1), :]\n",
        "            \n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            trm_loss = loss_fct(\n",
        "                trm_logits.reshape(-1, trm_logits.size(-1)),\n",
        "                shifted_labels.reshape(-1)\n",
        "            )\n",
        "            outputs.loss = outputs.loss + 0.3 * trm_loss\n",
        "        \n",
        "        return outputs\n",
        "    \n",
        "    def generate_with_thinking(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_new_tokens: int = 256,\n",
        "        temperature: float = 0.7,\n",
        "        do_sample: bool = True\n",
        "    ) -> str:\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.base_model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.base_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                do_sample=do_sample,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "print(\"SmolLMv3WithTRM defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. GSM8K Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GSM8K Dataset class defined\n"
          ]
        }
      ],
      "source": [
        "class GSM8KDataset(Dataset):\n",
        "    \"\"\"Dataset for GSM8K with <think> token support\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset,\n",
        "        tokenizer,\n",
        "        max_length: int = 512,\n",
        "        add_think_token: bool = True\n",
        "    ):\n",
        "        self.data = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.add_think_token = add_think_token\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        item = self.data[idx]\n",
        "        \n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "        \n",
        "        # Format with <think> token to trigger TRM reasoning\n",
        "        if self.add_think_token:\n",
        "            text = f\"Question: {question}\\nAnswer: <think> {answer}\"\n",
        "        else:\n",
        "            text = f\"Question: {question}\\nAnswer: {answer}\"\n",
        "        \n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
        "        \n",
        "        # Create labels\n",
        "        labels = input_ids.clone()\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "        \n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "print(\"GSM8K Dataset class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. PyTorch Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lightning Module defined\n"
          ]
        }
      ],
      "source": [
        "class SmolLMTRMLightningModule(pl.LightningModule):\n",
        "    \"\"\"PyTorch Lightning module for training\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"HuggingFaceTB/SmolLM3-3B\",\n",
        "        use_lora: bool = True,\n",
        "        lora_r: int = 16,\n",
        "        lora_alpha: int = 32,\n",
        "        num_latents: int = 256,\n",
        "        learning_rate: float = 2e-4,\n",
        "        weight_decay: float = 0.01,\n",
        "        warmup_steps: int = 100,\n",
        "        trm_kwargs: Optional[Dict] = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        self.model = SmolLMv3WithTRM(\n",
        "            model_name=model_name,\n",
        "            use_lora=use_lora,\n",
        "            lora_r=lora_r,\n",
        "            lora_alpha=lora_alpha,\n",
        "            num_latents=num_latents,\n",
        "            trm_kwargs=trm_kwargs or {}\n",
        "        )\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.total_steps = None\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, labels):\n",
        "        return self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            use_trm=True\n",
        "        )\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        outputs = self(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            labels=batch[\"labels\"]\n",
        "        )\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train/perplexity\", torch.exp(loss), on_step=False, on_epoch=True)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outputs = self(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            labels=batch[\"labels\"]\n",
        "        )\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
        "        self.log(\"val/perplexity\", torch.exp(loss), on_step=False, on_epoch=True, sync_dist=True)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            filter(lambda p: p.requires_grad, self.parameters()),\n",
        "            lr=self.learning_rate,\n",
        "            weight_decay=self.weight_decay\n",
        "        )\n",
        "        \n",
        "        if self.total_steps is None:\n",
        "            self.total_steps = self.trainer.estimated_stepping_batches\n",
        "        \n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=self.warmup_steps,\n",
        "            num_training_steps=self.total_steps\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"step\",\n",
        "                \"frequency\": 1\n",
        "            }\n",
        "        }\n",
        "\n",
        "print(\"Lightning Module defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training function defined\n"
          ]
        }
      ],
      "source": [
        "def train_gsm8k(\n",
        "    train_dataset,\n",
        "    test_dataset,\n",
        "    model_name: str = \"HuggingFaceTB/SmolLM3-3B\",\n",
        "    output_dir: str = \"./checkpoints\",\n",
        "    batch_size: int = 2,\n",
        "    num_epochs: int = 3,\n",
        "    learning_rate: float = 2e-4,\n",
        "    num_latents: int = 256,\n",
        "    accumulate_grad_batches: int = 4,\n",
        "    val_check_interval: float = 0.25,\n",
        "    precision: str = \"bf16-mixed\",\n",
        "    devices: int = 1,\n",
        "    wandb_project: str = \"smollm-trm-gsm8k\",\n",
        "    wandb_name: Optional[str] = None,\n",
        "):\n",
        "    \"\"\"Train on GSM8K dataset with provided train/test split\"\"\"\n",
        "    \n",
        "    print(f\"Train examples: {len(train_dataset)}\")\n",
        "    print(f\"Test examples: {len(test_dataset)}\")\n",
        "    \n",
        "    # Initialize model\n",
        "    pl_module = SmolLMTRMLightningModule(\n",
        "        model_name=model_name,\n",
        "        use_lora=True,\n",
        "        lora_r=16,\n",
        "        lora_alpha=32,\n",
        "        num_latents=num_latents,\n",
        "        learning_rate=learning_rate,\n",
        "        warmup_steps=100,\n",
        "        trm_kwargs={\n",
        "            \"n_layers\": 2,\n",
        "            \"n_latent_steps\": 4,\n",
        "            \"n_deep_recursions\": 2,\n",
        "            \"n_supervision_steps\": 4,\n",
        "            \"compression_heads\": 8\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset_wrapped = GSM8KDataset(\n",
        "        train_dataset,\n",
        "        pl_module.model.tokenizer,\n",
        "        max_length=512,\n",
        "        add_think_token=True\n",
        "    )\n",
        "    \n",
        "    test_dataset_wrapped = GSM8KDataset(\n",
        "        test_dataset,\n",
        "        pl_module.model.tokenizer,\n",
        "        max_length=512,\n",
        "        add_think_token=True\n",
        "    )\n",
        "    \n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset_wrapped,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,  # Set to 0 to avoid multiprocessing issues in Jupyter\n",
        "        pin_memory=False  # Disable pin_memory for MPS compatibility\n",
        "    )\n",
        "    \n",
        "    test_loader = DataLoader(\n",
        "        test_dataset_wrapped,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,  # Set to 0 to avoid multiprocessing issues in Jupyter\n",
        "        pin_memory=False  # Disable pin_memory for MPS compatibility\n",
        "    )\n",
        "    \n",
        "    # Setup wandb logger\n",
        "    wandb_logger = WandbLogger(\n",
        "        project=wandb_project,\n",
        "        name=wandb_name,\n",
        "        log_model=True\n",
        "    )\n",
        "    \n",
        "    # Setup callbacks\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath=output_dir,\n",
        "        filename=\"smollm-trm-gsm8k-{epoch:02d}-{val/loss:.4f}\",\n",
        "        monitor=\"val/loss\",\n",
        "        mode=\"min\",\n",
        "        save_top_k=3,\n",
        "        save_last=True,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    early_stop_callback = EarlyStopping(\n",
        "        monitor=\"val/loss\",\n",
        "        patience=3,\n",
        "        mode=\"min\",\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
        "    \n",
        "    # Create trainer\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=num_epochs,\n",
        "        accelerator=\"auto\",\n",
        "        devices=devices,\n",
        "        precision=precision,\n",
        "        accumulate_grad_batches=accumulate_grad_batches,\n",
        "        gradient_clip_val=1.0,\n",
        "        val_check_interval=val_check_interval,\n",
        "        logger=wandb_logger,\n",
        "        callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],\n",
        "        log_every_n_steps=10,\n",
        "        enable_progress_bar=True,\n",
        "        enable_model_summary=True\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Starting training on GSM8K...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "    \n",
        "    trainer.fit(\n",
        "        pl_module,\n",
        "        train_dataloaders=train_loader,\n",
        "        val_dataloaders=test_loader\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"Training complete!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Best checkpoint: {checkpoint_callback.best_model_path}\")\n",
        "    print(f\"Best test loss: {checkpoint_callback.best_model_score:.4f}\")\n",
        "    \n",
        "    return trainer, pl_module\n",
        "\n",
        "print(\"Training function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluation Functions\n",
        "\n",
        "Extract numerical answers and compute accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation functions defined\n"
          ]
        }
      ],
      "source": [
        "def extract_answer(text: str) -> Optional[str]:\n",
        "    \"\"\"Extract numerical answer from GSM8K format\"\"\"\n",
        "    # GSM8K answers are in format \"#### 123\"\n",
        "    match = re.search(r'####\\s*([\\d,\\.]+)', text)\n",
        "    if match:\n",
        "        return match.group(1).replace(',', '')\n",
        "    \n",
        "    # Also try to find last number in text\n",
        "    numbers = re.findall(r'[\\d,]+\\.?\\d*', text)\n",
        "    if numbers:\n",
        "        return numbers[-1].replace(',', '')\n",
        "    \n",
        "    return None\n",
        "\n",
        "\n",
        "def evaluate_gsm8k(model, test_dataset, max_samples: Optional[int] = None, batch_size: int = 1):\n",
        "    \"\"\"\n",
        "    Evaluate model on GSM8K test set.\n",
        "    \n",
        "    Args:\n",
        "        model: The trained SmolLMv3WithTRM model\n",
        "        test_dataset: GSM8K test dataset\n",
        "        max_samples: Number of samples to evaluate (None for all)\n",
        "        batch_size: Batch size for generation\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with accuracy and examples\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.base_model.eval()\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    examples = []\n",
        "    \n",
        "    num_to_eval = len(test_dataset) if max_samples is None else min(max_samples, len(test_dataset))\n",
        "    print(f\"Evaluating on {num_to_eval} examples...\")\n",
        "    \n",
        "    for i in range(num_to_eval):\n",
        "        item = test_dataset[i]\n",
        "        question = item['question']\n",
        "        true_answer = extract_answer(item['answer'])\n",
        "        \n",
        "        # Create prompt with <think> token\n",
        "        prompt = f\"Question: {question}\\nAnswer: <think>\"\n",
        "        \n",
        "        # Generate answer\n",
        "        generated = model.generate_with_thinking(\n",
        "            prompt,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.1,  # Low temperature for more deterministic answers\n",
        "            do_sample=True\n",
        "        )\n",
        "        \n",
        "        # Extract predicted answer\n",
        "        pred_answer = extract_answer(generated)\n",
        "        \n",
        "        # Check if correct\n",
        "        is_correct = (pred_answer == true_answer) if (pred_answer and true_answer) else False\n",
        "        \n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        # Save first 10 examples\n",
        "        if i < 10:\n",
        "            examples.append({\n",
        "                'question': question,\n",
        "                'true_answer': true_answer,\n",
        "                'predicted_answer': pred_answer,\n",
        "                'generated_text': generated,\n",
        "                'correct': is_correct\n",
        "            })\n",
        "        \n",
        "        # Progress\n",
        "        if (i + 1) % 10 == 0:\n",
        "            acc = correct / total * 100\n",
        "            print(f\"  {i+1}/{num_to_eval} - Accuracy: {acc:.2f}%\")\n",
        "    \n",
        "    accuracy = correct / total * 100\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'correct': correct,\n",
        "        'total': total,\n",
        "        'examples': examples\n",
        "    }\n",
        "\n",
        "print(\"Evaluation functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Configure and Start Training\n",
        "\n",
        "The training will use the 80/20 split we created earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Configuration:\n",
            "  Train size: 5979 examples\n",
            "  Test size: 1494 examples\n",
            "  Epochs: 3\n",
            "  Batch size: 2\n",
            "  Learning rate: 0.0002\n",
            "\n",
            "Ready to train! Run the next cell to start.\n"
          ]
        }
      ],
      "source": [
        "# Training configuration\n",
        "config = {\n",
        "    \"train_dataset\": gsm8k_train,  # 80% of GSM8K train set\n",
        "    \"test_dataset\": gsm8k_test,    # 20% of GSM8K train set\n",
        "    \"model_name\": \"HuggingFaceTB/SmolLM3-3B\",\n",
        "    \"batch_size\": 2,\n",
        "    \"num_epochs\": 3,\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"num_latents\": 256,\n",
        "    \"accumulate_grad_batches\": 4,\n",
        "    \"precision\": \"bf16-mixed\",\n",
        "    \"devices\": 1,\n",
        "    \"wandb_project\": \"smollm-trm-gsm8k\",\n",
        "    \"wandb_name\": \"gsm8k-80-20-split\",\n",
        "    \"output_dir\": \"./checkpoints\",\n",
        "}\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(f\"  Train size: {len(config['train_dataset'])} examples\")\n",
        "print(f\"  Test size: {len(config['test_dataset'])} examples\")\n",
        "print(f\"  Epochs: {config['num_epochs']}\")\n",
        "print(f\"  Batch size: {config['batch_size']}\")\n",
        "print(f\"  Learning rate: {config['learning_rate']}\")\n",
        "print(\"\\nReady to train! Run the next cell to start.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train examples: 5979\n",
            "Test examples: 1494\n",
            "Loading HuggingFaceTB/SmolLM3-3B...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.10s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LoRA adapters applied:\n",
            "trainable params: 7,667,712 || all params: 3,082,766,336 || trainable%: 0.2487\n",
            "\n",
            "Initializing TRM with 256 latents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model initialized\n",
            "  <think> token ID: 128002\n",
            "  TRM parameters: 151.59M\n",
            "\n",
            "======================================================================\n",
            "Starting training on GSM8K...\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20251010_190717-lmjbeixo</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/anonx/smollm-trm-gsm8k/runs/lmjbeixo' target=\"_blank\">gsm8k-80-20-split</a></strong> to <a href='https://wandb.ai/anonx/smollm-trm-gsm8k' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/anonx/smollm-trm-gsm8k' target=\"_blank\">https://wandb.ai/anonx/smollm-trm-gsm8k</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/anonx/smollm-trm-gsm8k/runs/lmjbeixo' target=\"_blank\">https://wandb.ai/anonx/smollm-trm-gsm8k/runs/lmjbeixo</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading `train_dataloader` to estimate number of stepping batches.\n",
            "/Users/neosapien/Development/llm-trm/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=10` in the `DataLoader` to improve performance.\n",
            "/Users/neosapien/Development/llm-trm/.venv/lib/python3.13/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
            "\n",
            "  | Name  | Type            | Params | Mode \n",
            "--------------------------------------------------\n",
            "0 | model | SmolLMv3WithTRM | 3.2 B  | train\n",
            "--------------------------------------------------\n",
            "159 M     Trainable params\n",
            "3.1 B     Non-trainable params\n",
            "3.2 B     Total params\n",
            "12,937.437Total estimated model params size (MB)\n",
            "1476      Modules in train mode\n",
            "475       Modules in eval mode\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/neosapien/Development/llm-trm/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=10` in the `DataLoader` to improve performance.\n",
            "/Users/neosapien/Development/llm-trm/.venv/lib/python3.13/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/neosapien/Development/llm-trm/.venv/lib/python3.13/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 475 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 1/2990 [00:48<40:22:16,  0.02it/s, v_num=eixo, train/loss_step=nan.0]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb-core(77045) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "wandb-core(77051) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "wandb-core(77052) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "wandb-core(77057) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "wandb-core(77061) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "wandb-core(77067) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "wandb-core(77068) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "wandb-core(77075) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "wandb-core(77080) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "wandb-core(77086) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "wandb-core(77129) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "wandb-core(77147) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "wandb-core(77167) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "wandb-core(77198) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        }
      ],
      "source": [
        "# Start training on GSM8K\n",
        "trainer, model = train_gsm8k(**config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Evaluate on Test Set (20% split)\n",
        "\n",
        "Test the trained model on our 20% test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on our 20% test split\n",
        "results = evaluate_gsm8k(\n",
        "    model.model,\n",
        "    gsm8k_test,\n",
        "    max_samples=100,  # Evaluate on 100 examples (test set has ~1,495)\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Test Set Results (20% split)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Accuracy: {results['accuracy']:.2f}%\")\n",
        "print(f\"Correct: {results['correct']}/{results['total']}\")\n",
        "print(\"\\nExample Predictions:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, ex in enumerate(results['examples'][:5]):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Question: {ex['question']}\")\n",
        "    print(f\"True Answer: {ex['true_answer']}\")\n",
        "    print(f\"Predicted Answer: {ex['predicted_answer']}\")\n",
        "    print(f\"Correct: {ex['correct']}\")\n",
        "    print(\"-\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Final Evaluation on GSM8K Official Test Set (Optional)\n",
        "\n",
        "Evaluate on the official GSM8K test set (1,319 examples) - use this for final benchmarking only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on official GSM8K test set (1,319 examples)\n",
        "# This is the standard benchmark test set - use sparingly to avoid overfitting\n",
        "\n",
        "final_results = evaluate_gsm8k(\n",
        "    model.model,\n",
        "    gsm8k_final_test,\n",
        "    max_samples=100,  # Test on 100 examples (full set has 1,319)\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Final Evaluation on Official GSM8K Test Set\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Accuracy: {final_results['accuracy']:.2f}%\")\n",
        "print(f\"Correct: {final_results['correct']}/{final_results['total']}\")\n",
        "print(\"\\nNote: This is evaluated on the official GSM8K test set\")\n",
        "print(\"Use this for final benchmarking only to avoid overfitting\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Test Individual Problems\n",
        "\n",
        "Try the model on specific math problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on custom problems\n",
        "test_problems = [\n",
        "    \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\",\n",
        "    \"A restaurant served 9 pizzas during lunch and 6 during dinner today. How many pizzas were served in total?\",\n",
        "    \"If John has 5 apples and buys 7 more, then gives away 3, how many apples does he have left?\",\n",
        "]\n",
        "\n",
        "print(\"Testing on custom problems:\\n\")\n",
        "for i, problem in enumerate(test_problems):\n",
        "    prompt = f\"Question: {problem}\\nAnswer: <think>\"\n",
        "    \n",
        "    response = model.model.generate_with_thinking(\n",
        "        prompt,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.1\n",
        "    )\n",
        "    \n",
        "    # Extract just the answer part\n",
        "    answer_part = response.split(\"<think>\")[-1] if \"<think>\" in response else response\n",
        "    \n",
        "    print(f\"Problem {i+1}:\")\n",
        "    print(f\"Q: {problem}\")\n",
        "    print(f\"A: {answer_part.strip()}\")\n",
        "    print(\"-\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Save Trained Model\n",
        "\n",
        "Save the model for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model\n",
        "save_path = \"./gsm8k_model\"\n",
        "\n",
        "# Save base model with LoRA\n",
        "model.model.base_model.save_pretrained(save_path)\n",
        "\n",
        "# Save TRM weights\n",
        "torch.save(model.model.trm.state_dict(), f\"{save_path}/trm_weights.pt\")\n",
        "\n",
        "# Save tokenizer\n",
        "model.model.tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"Model saved to {save_path}\")\n",
        "print(\"\\nTo load later:\")\n",
        "print(\"  model = SmolLMv3WithTRM(model_name=save_path)\")\n",
        "print(f\"  model.trm.load_state_dict(torch.load('{save_path}/trm_weights.pt'))\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
