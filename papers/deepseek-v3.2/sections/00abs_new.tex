

\begin{abstract}


We introduce \newmodel, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of \newmodel{} are as follows: \textbf{(1)  DeepSeek Sparse Attention (DSA)}:
We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios.
\textbf{(2) Scalable Reinforcement Learning Framework}: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, \highmodel{}, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving \textcolor{gold}{gold-medal} performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI).
\textbf{(3) Large-Scale Agentic Task Synthesis Pipeline}: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.


\end{abstract}
