\section{Introduction}

The release of reasoning models \citep{o1, deepseekr1} marked a pivotal moment in the evolution of Large Language Models (LLMs), catalyzing a substantial leap in overall performance across the verifiable fields. Since this milestone, the capabilities of LLMs have advanced rapidly. However, a distinct divergence has emerged in the past months. While the open-source community \citep{yang2025qwen3technicalreport, zeng2025glm,MiniMax-M2, k2-thinking} continues to make strides, the performance trajectory of closed-source proprietary models \citep{gpt-5, sonnet-4.5, comanici2025gemini} has accelerated at a significantly steeper rate. Consequently, rather than converging, the performance gap between closed-source and open-source models appears to be widening, with proprietary systems demonstrating increasingly superior capabilities in complex tasks.

Through our analysis, we identify three critical deficiencies that limit the capability of open-source models in complex tasks. First, architecturally, the predominant reliance on vanilla attention \citep{vaswani2017attention} mechanisms severely constrains efficiency for long sequences. This inefficiency poses a substantial obstacle to both scalable deployment and effective post-training. Second, regarding resource allocation, open-source models suffer from insufficient computational investment during the post-training phase, limiting their performance on hard tasks. Finally, in the context of AI agents, open-source models demonstrate a marked lag in generalization and instruction-following capabilities compared to their proprietary counterparts \citep{mcpmark,mcpuniverse,li2025tool}, hindering their effectiveness in real deployment.
%, such as the International Olympiad in Informatics (IOI)\footnote{\url{https://ioinformatics.org/}} and the International Mathematical Olympiad (IMO)\footnote{\url{https://www.imo-official.org/}}

To address these critical limitations, we first introduce DSA, a highly efficient attention mechanism designed to substantially reduce computational complexity. This architecture effectively addresses the efficiency bottleneck, preserving model performance even in long-context scenarios. Second, we develop a stable and scalable RL protocol that allows for significant computational expansion during the post-training phase. Notably, this framework allocates a post-training computational budget exceeding 10\% of the pre-training cost, unlocking advanced capabilities. Thirdly, we propose a novel pipeline to foster generalizable reasoning in tool-use scenarios. First, we implement a cold-start phase utilizing the DeepSeek-V3 \citep{deepseekv3} methodology to unify reasoning and tool-use within single trajectories. Subsequently, we advance to large-scale agentic task synthesis, where we generate over 1,800 distinct environments and 85,000 complex prompts. This extensive synthesized data drives the RL process, significantly enhancing the model's generalization and instruction-following capability in the agent context. 

\newmodel{} achieves similar performance with Kimi-k2-thinking and GPT-5 across multiple reasoning benchmarks. Furthermore, \newmodel{} significantly advances the agentic capabilities of open models, demonstrating exceptional proficiency on the long-tail agent tasks introduced in \citet{mcpmark,mcpuniverse,li2025tool}. \newmodel{} emerges as a highly cost-efficient alternative in agent scenarios, significantly narrowing the performance gap between open and frontier proprietary models while incurring substantially lower costs.
Notably, with the aim of pushing the boundaries of open models in the reasoning domain, we relaxed the length constraints to develop \highmodel{}. As a result, \highmodel{} achieves performance parity with the leading closed-source system, Gemini-3.0-Pro \citep{gemini3}. It shows gold-medal performance in the IOI 2025, ICPC World Final 2025, IMO 2025, and CMO 2025.

