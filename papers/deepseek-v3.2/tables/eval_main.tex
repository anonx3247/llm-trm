\begin{table}[htbp]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{1.9pt}
    \caption{ Comparison between \newmodel{} and closed/open models. For open models, we just compare with models supports thinking in tooluse. 
    Numbers in bold represent the best scores within each model class (open-source and closed-source).  The $\tau^2$-Bench result is computed by the average of each category. Regarding BrowseComp, the performance with the context management technique is noted with *.
      }
    \begin{tabular}{@{}c l | c  c  c | c c c c @{}}
    \toprule
    & \multirow{2}{*}{\centering \textbf{Benchmark {\tiny (Metric)}}}  & \textbf{Claude-4.5-}  & \textbf{GPT-5}& \textbf{Gemini-3.0} & \textbf{Kimi-K2} & \textbf{MiniMax} & \textbf{\newmodel} \\
    & & \textbf{Sonnet}  & \textbf{High} & \textbf{Pro} & \textbf{Thinking}& \textbf{M2} &\textbf{Thinking} \\

    \midrule
    \multirow{3}{*}{English}
    & MMLU-Pro {\tiny (EM)} & 88.2 & 87.5 & \textbf{90.1} & 84.6 & 82.0 & \textbf{85.0}\\

    & GPQA Diamond {\tiny (Pass@1)} &83.4 & 85.7& \textbf{91.9} & \textbf{84.5} & 77.7 &82.4\\
    & HLE {\tiny (Pass@1)} &13.7 & 26.3 & \textbf{37.7} & 23.9 & 12.5 & \textbf {25.1}\\
    \midrule
    \multirow{2}{*}{Code} & LiveCodeBench {\tiny (Pass@1-COT)}  & 64.0& 84.5& \textbf{90.7}& 82.6& 83.0&   \textbf{83.3} \\
    & Codeforces {\tiny (Rating)} & 1480 &2537& \textbf{2708}& -& -&2386 \\
    % & Aider-Polyglot {\tiny (Acc.)} & & & & & & \\
    \midrule
        \multirow{4}{*}{Math} & AIME 2025 {\tiny (Pass@1)}  & 87.0 & 94.6 & \textbf{95.0} & \textbf{94.5} & 78.3 & 93.1 \\
    & HMMT Feb 2025 {\tiny (Pass@1)}  & 79.2 & 88.3 & \textbf{97.5} & 89.4 & - &\textbf{92.5} \\
    & HMMT Nov 2025 {\tiny (Pass@1)}  & 81.7 & 89.2 & \textbf{93.3} & 89.2 & - &  \textbf{90.2}\\
    & IMOAnswerBench {\tiny (Pass@1)}  & - & 76.0 & \textbf{83.3} & \textbf{78.6} & -  &78.3  \\
    \midrule
     \multirow{3}{*}{Code Agent} & Terminal Bench 2.0 {\tiny (Acc)}  &42.8& 35.2& \textbf{54.2} & 35.7& 30.0 & \textbf{46.4}\\
      & SWE Verified {\tiny (Resolved)}  & \textbf{77.2} & 74.9&76.2 & 71.3& 69.4 &  \textbf{73.1}\\
      & SWE Multilingual {\tiny (Resolved)}  &\textbf{68.0} &55.3 &- & 61.1 & 56.5	 & \textbf{70.2}\\  \midrule

    \multirow{3}{*}{Search Agent} & BrowseComp {\tiny (Pass@1)}  & 24.1 & \textbf{54.9} & - & -/60.2* & 44.0& \textbf{51.4/67.6}* \\
    & BrowseCompZh {\tiny (Pass@1)}  & 42.4 & 63.0 & - & 62.3 & 48.5 & \textbf{65.0} \\
    & HLE {\tiny (Pass@1)}  &32.0 & 35.2 & \textbf{45.8} & \textbf{44.9} & 31.8 & 40.8 \\ \midrule

         \multirow{4}{*}{ToolUse}  &$\tau^2$-Bench{\tiny (Pass@1)} & 84.7& 80.2& \textbf{85.4} &74.3 & 76.9 & \textbf{80.3}\\
         & MCP-Universe \tiny{(Success Rate)}  & 46.5 & 47.9 & \textbf{50.7} & 35.6 &29.4 & \textbf{45.9} \\
      &  MCP-Mark {\tiny (Pass@1)} &33.3 & \textbf{50.9} & 43.1 &20.4 &24.4 & \textbf{38.0}\\
      & Tool-Decathlon {\tiny (Pass@1)}  & \textbf{38.6} & 29.0 &36.4 & 17.6& 16.0& \textbf{35.2} \\  
      
    \bottomrule
    \end{tabular}
    
    \label{tab:main}
\end{table}
