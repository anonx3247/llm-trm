"""
Phase 2: TRM Iteration Training (Sequence-to-Sequence)

Train the TRM to replace LLM's chain-of-thought with latent iterations.

Goal: TRM learns to map context sequence -> context + reasoning token
      Input:  [B, L, D'] (compressed context before thinking)
      Output: [B, L+1, D'] (context + appended reasoning token)
      Target: The reasoning token at position L+1 should match
              what was at position L+M after M thinking tokens

Training follows the TRM paper methodology:
1. Input: Compressed context sequences (variable length)
2. Target: Compressed hidden state after thinking
3. TRM iterates to produce reasoning token:
   - Latent recursion: z = net(x + y + z) repeated n times
   - Prediction update: y = net(y + z)
   - Deep recursion: T-1 without gradients, 1 with
4. Loss: ||output[:, -1, :] - target||^2

Uses data generated by phase2_datagen.py

Usage:
    python -m src.train.phase2_trm \\
        --data_path ./data/hidden_pairs/hidden_pairs.pt \\
        --compressor_checkpoint anonx3247/llm-trm-compressor \\
        --output_dir ./checkpoints/phase2 \\
        --num_epochs 100
"""

import os
import sys
from dataclasses import dataclass
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
from huggingface_hub import hf_hub_download
from torch.nn.utils.rnn import pad_sequence
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

from src.models.compression import DimensionCompressor
from src.models.trm import RecursiveReasoningBase, TinyRecursiveNetwork
from src.train.phase1_compressor import Phase1Config

# Optional wandb import
try:
    import wandb

    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    wandb = None  # type: ignore[assignment]


@dataclass
class Phase2Config:
    """Configuration for Phase 2 TRM training"""

    # Data
    data_path: str = "./data/hidden_pairs/hidden_pairs.pt"

    # Compressor (from Phase 1)
    compressor_checkpoint: str = "anonx3247/llm-trm-compressor"

    # Model (from TRM paper)
    hidden_size: int = 2048  # SmolLM3 actual hidden size
    d_compressed: int = 256  # Must match compressor
    n_layers: int = 2  # TRM paper: 2 layers is optimal
    n_heads: int = 8
    n_latent_steps: int = 6  # n in paper
    n_deep_recursions: int = 3  # T in paper
    n_supervision_steps: int = 8  # N_sup in paper (16 in paper, adjustable via CLI)
    dropout: float = 0.1  # Dropout for stability

    # Halting (ACT - Adaptive Computation Time)
    use_halting: bool = True  # Train halt head during Phase 2
    halt_threshold: float = 0.95  # Cosine sim threshold for "correct" prediction
    halt_loss_weight: float = 0.5  # Weight for halting loss (0.5 in paper)
    early_stop_training: bool = False  # Whether to early stop during training

    # Training (from TRM paper hyperparameters)
    batch_size: int = 32  # Smaller due to variable length sequences
    learning_rate: float = 1e-4
    weight_decay: float = 0.1  # 0.1 for reasoning tasks
    num_epochs: int = 100
    warmup_ratio: float = 0.1
    max_grad_norm: float = 1.0
    gradient_accumulation_steps: int = 1

    # EMA (critical for stability per paper)
    use_ema: bool = True
    ema_decay: float = 0.999

    # Output
    output_dir: str = "./checkpoints/phase2"
    log_steps: int = 10
    save_steps: int = 1000

    # Wandb
    use_wandb: bool = True
    wandb_project: str = "llm-trm-phase2"
    wandb_run_name: str | None = None

    # Reproducibility
    seed: int = 42


class HiddenStateSequenceDataset(Dataset):
    """Dataset for variable-length hidden state sequences."""

    def __init__(
        self,
        hidden_pre_list: list[torch.Tensor],  # List of [L_i, D]
        hidden_post: torch.Tensor,  # [N, D]
        compressor: DimensionCompressor,
        device: torch.device,
    ):
        """
        Args:
            hidden_pre_list: List of context sequences, each [L_i, D]
            hidden_post: Target states [N, D]
            compressor: Pre-trained compressor (frozen)
            device: Device for compression
        """
        self.hidden_post = hidden_post
        self.compressor = compressor
        self.device = device

        # Pre-compress all sequences to save compute during training
        self.compressed_pre: list[torch.Tensor] = []
        self.compressed_post: list[torch.Tensor] = []

        print("Pre-compressing sequences...")
        with torch.no_grad():
            for i, seq in enumerate(tqdm(hidden_pre_list, desc="Compressing")):
                # Compress context: [L, D] -> [L, D']
                seq_device = seq.to(device)
                compressed_seq = compressor(seq_device).cpu()
                self.compressed_pre.append(compressed_seq)

                # Compress target: [D] -> [D']
                target = hidden_post[i : i + 1].to(device)  # [1, D]
                compressed_target = compressor(target).cpu().squeeze(0)  # [D']
                self.compressed_post.append(compressed_target)

        # Stack targets since they're all same size
        self.compressed_post_tensor = torch.stack(self.compressed_post)  # [N, D']
        print(f"Compressed {len(self.compressed_pre)} sequences")

        # Validate data - check for NaN/Inf
        for i, seq in enumerate(self.compressed_pre):
            if torch.isnan(seq).any() or torch.isinf(seq).any():
                print(f"Warning: NaN/Inf in compressed_pre[{i}]")
        if torch.isnan(self.compressed_post_tensor).any():
            print("Warning: NaN in compressed_post_tensor")

        # Print data statistics
        all_pre = torch.cat(self.compressed_pre, dim=0)
        print(
            f"  Pre stats: mean={all_pre.mean():.4f}, std={all_pre.std():.4f}, "
            f"min={all_pre.min():.4f}, max={all_pre.max():.4f}"
        )
        print(
            f"  Post stats: mean={self.compressed_post_tensor.mean():.4f}, "
            f"std={self.compressed_post_tensor.std():.4f}, "
            f"min={self.compressed_post_tensor.min():.4f}, "
            f"max={self.compressed_post_tensor.max():.4f}"
        )

    def __len__(self) -> int:
        return len(self.compressed_pre)

    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Returns:
            context: Compressed context sequence [L, D']
            target: Compressed target state [D']
        """
        return self.compressed_pre[idx], self.compressed_post_tensor[idx]


def collate_variable_length(
    batch: list[tuple[torch.Tensor, torch.Tensor]],
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Collate function for variable-length sequences.

    Args:
        batch: List of (context [L_i, D'], target [D']) tuples

    Returns:
        contexts: Padded contexts [B, max_L, D']
        targets: Stacked targets [B, D']
        mask: Attention mask [B, max_L] (1 = valid, 0 = padding)
    """
    contexts_tuple, targets_tuple = zip(*batch, strict=True)
    contexts = list(contexts_tuple)
    targets = list(targets_tuple)

    # Get sequence lengths
    lengths = [c.size(0) for c in contexts]

    # Pad sequences (pad_sequence pads to max length in batch)
    # Default padding is 0, which is fine
    contexts_padded = pad_sequence(contexts, batch_first=True)  # [B, max_L, D']

    # Create attention mask
    max_len = contexts_padded.size(1)
    mask = torch.zeros(len(lengths), max_len)
    for i, length in enumerate(lengths):
        mask[i, :length] = 1

    # Stack targets
    targets_stacked = torch.stack(targets)  # [B, D']

    return contexts_padded, targets_stacked, mask


class SequenceTRM(RecursiveReasoningBase):
    """
    TRM for sequence-to-sequence reasoning.

    Takes [B, L, D'] context and outputs [B, L+1, D'] with appended reasoning token.
    """

    def __init__(
        self,
        d_compressed: int = 256,
        n_layers: int = 2,
        n_heads: int = 8,
        n_latent_steps: int = 6,
        n_deep_recursions: int = 3,
        dropout: float = 0.1,
    ):
        super().__init__()

        self.d_compressed = d_compressed
        self.n_latent_steps = n_latent_steps
        self.n_deep_recursions = n_deep_recursions

        # TRM network operates on sequences
        self.net = TinyRecursiveNetwork(
            d_model=d_compressed, n_layers=n_layers, n_heads=n_heads, dropout=dropout
        )

        # Learnable reasoning token that gets appended and refined
        self.reasoning_token = nn.Parameter(torch.randn(1, 1, d_compressed) * 0.02)

        # Halting mechanism (required by base class, trained via RL in Phase 3)
        self.halt_head = nn.Linear(d_compressed, 1)

        # Output normalization for stability
        self.output_norm = nn.RMSNorm(d_compressed)

    def forward(
        self, x: torch.Tensor, mask: torch.Tensor | None = None, n_steps: int = 1
    ) -> torch.Tensor:
        """
        Run TRM reasoning on context sequence.

        Args:
            x: Context sequence [B, L, D']
            mask: Attention mask [B, L] (1 = valid, 0 = padding)
            n_steps: Number of supervision steps

        Returns:
            output: [B, L+1, D'] with appended reasoning token
        """
        B, L, D = x.shape

        # Expand reasoning token for batch
        reasoning = self.reasoning_token.expand(B, 1, D)  # [B, 1, D']

        # Append reasoning token to context
        # x_aug: [B, L+1, D']
        x_aug = torch.cat([x, reasoning], dim=1)

        # Create augmented mask if provided
        if mask is not None:
            # Add 1 for reasoning token position
            mask_aug = torch.cat([mask, torch.ones(B, 1, device=mask.device)], dim=1)  # [B, L+1]
        else:
            mask_aug = None

        # Initialize y and z for recursion
        y = torch.zeros_like(x_aug)
        z = torch.zeros_like(x_aug)

        # Run supervision steps with deep recursion
        for _ in range(n_steps):
            y, z = self._deep_recursion_with_mask(x_aug, y, z, mask_aug)

        # Output: original context + refined reasoning token
        # y contains the refined sequence [B, L+1, D']
        return y

    def forward_with_halting(
        self,
        x: torch.Tensor,
        mask: torch.Tensor | None = None,
        n_steps: int = 1,
        early_stop: bool = False,
    ) -> tuple[torch.Tensor, list[torch.Tensor], int]:
        """
        Run TRM with halting probabilities at each step.

        Args:
            x: Context sequence [B, L, D']
            mask: Attention mask [B, L]
            n_steps: Max supervision steps
            early_stop: Whether to stop early when halt_prob > 0.5

        Returns:
            output: [B, L+1, D']
            halt_probs: List of [B, 1] halt probabilities at each step
            steps_taken: Number of steps actually taken
        """
        B, L, D = x.shape

        # Setup (same as forward)
        reasoning = self.reasoning_token.expand(B, 1, D)
        x_aug = torch.cat([x, reasoning], dim=1)

        if mask is not None:
            mask_aug = torch.cat([mask, torch.ones(B, 1, device=mask.device)], dim=1)
        else:
            mask_aug = None

        y = torch.zeros_like(x_aug)
        z = torch.zeros_like(x_aug)

        halt_probs = []
        steps_taken = 0

        for _step in range(n_steps):
            y, z = self._deep_recursion_with_mask(x_aug, y, z, mask_aug)
            steps_taken += 1

            # Compute halt probability from reasoning token
            reasoning_state = y[:, -1, :]  # [B, D']
            halt_logits = self.halt_head(reasoning_state)  # [B, 1]
            halt_prob = torch.sigmoid(halt_logits)  # [B, 1]
            halt_probs.append(halt_prob)

            # Early stopping
            if early_stop and halt_prob.mean() > 0.5:
                break

        return y, halt_probs, steps_taken

    def _deep_recursion_with_mask(
        self,
        x: torch.Tensor,
        y: torch.Tensor,
        z: torch.Tensor,
        mask: torch.Tensor | None,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Deep recursion with optional masking.

        From TRM paper:
        - T-1 iterations without gradients
        - 1 iteration with gradients
        """
        # T-1 iterations without gradients
        with torch.no_grad():
            for _ in range(self.n_deep_recursions - 1):
                y, z = self._latent_recursion_with_mask(x, y, z, mask)

        # 1 iteration with gradients
        y, z = self._latent_recursion_with_mask(x, y, z, mask)

        return y, z

    def _latent_recursion_with_mask(
        self,
        x: torch.Tensor,
        y: torch.Tensor,
        z: torch.Tensor,
        mask: torch.Tensor | None,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Latent recursion from TRM paper (Algorithm 2).

        for i in range(n):
            z = net(x + y + z)  # latent reasoning
        y = net(y + z)  # refine answer (no x!)

        Added normalization after each step for stability.
        """
        # Latent steps with normalization for stability
        for _ in range(self.n_latent_steps):
            z = self.net(x + y + z, mask=mask)
            z = self.output_norm(z)  # Normalize to prevent explosion

        # Answer refinement (NOTE: no x here, as per paper)
        y = self.net(y + z, mask=mask)
        y = self.output_norm(y)  # Normalize output

        return y, z


class EMA:
    """Exponential Moving Average for model weights."""

    def __init__(self, model: nn.Module, decay: float = 0.999):
        self.model = model
        self.decay = decay
        self.shadow: dict[str, torch.Tensor] = {}
        self.backup: dict[str, torch.Tensor] = {}

        # Initialize shadow weights
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()

    def update(self) -> None:
        """Update shadow weights with current model weights."""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = self.decay * self.shadow[name] + (1 - self.decay) * param.data

    def apply_shadow(self) -> None:
        """Apply shadow weights to model (for evaluation)."""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data.clone()
                param.data = self.shadow[name]

    def restore(self) -> None:
        """Restore original weights after evaluation."""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]
        self.backup = {}


class TRMSequenceTrainer:
    """
    Phase 2 trainer: TRM learns sequence-to-sequence reasoning.

    Training loop:
    1. Load pre-trained compressor (frozen)
    2. Pre-compress all sequences
    3. TRM takes [B, L, D'] context, outputs [B, L+1, D']
    4. Loss: MSE between output[:, -1, :] (reasoning token) and target
    """

    def __init__(self, config: Phase2Config):
        self.config = config
        self.device = self._get_device()
        self.best_loss = float("inf")

        # Create output directory
        Path(config.output_dir).mkdir(parents=True, exist_ok=True)

        # Initialize components
        self._load_compressor()
        self._init_trm()
        self._init_optimizer()
        self._init_ema()
        self._init_wandb()

    def _get_device(self) -> torch.device:
        """Determine device to use."""
        if torch.cuda.is_available():
            return torch.device("cuda")
        elif torch.backends.mps.is_available():
            return torch.device("mps")
        return torch.device("cpu")

    def _load_compressor(self) -> None:
        """Load pre-trained compressor from Phase 1."""
        ckpt_source = self.config.compressor_checkpoint

        # Download from HF Hub if needed
        if "/" in ckpt_source and not os.path.exists(ckpt_source):
            print(f"Downloading compressor from HF Hub: {ckpt_source}...")
            ckpt_path = hf_hub_download(repo_id=ckpt_source, filename="compressor.pt")
        else:
            ckpt_path = ckpt_source

        # Fix pickle compatibility
        sys.modules["__main__"].Phase1Config = Phase1Config  # type: ignore[attr-defined]
        checkpoint = torch.load(ckpt_path, map_location=self.device, weights_only=False)

        # Get config from checkpoint
        ckpt_config = checkpoint.get("config")
        if ckpt_config:
            self.config.hidden_size = ckpt_config.hidden_size
            self.config.d_compressed = ckpt_config.d_compressed
            print(
                f"Loaded compressor config: hidden_size={self.config.hidden_size}, "
                f"d_compressed={self.config.d_compressed}"
            )

        # Initialize compressor
        self.compressor = DimensionCompressor(
            d_model=self.config.hidden_size,
            d_compressed=self.config.d_compressed,
        )

        # Load state dict (handle torch.compile prefix)
        state_dict = checkpoint["compressor"]
        state_dict = {k.replace("_orig_mod.", ""): v for k, v in state_dict.items()}
        self.compressor.load_state_dict(state_dict)
        self.compressor.to(self.device)
        self.compressor.eval()

        # Freeze compressor
        for param in self.compressor.parameters():
            param.requires_grad = False

        print(
            f"Compressor loaded and frozen. "
            f"Compression ratio: {self.config.hidden_size / self.config.d_compressed:.1f}x"
        )

    def _init_trm(self) -> None:
        """Initialize TRM model."""
        self.trm = SequenceTRM(
            d_compressed=self.config.d_compressed,
            n_layers=self.config.n_layers,
            n_heads=self.config.n_heads,
            n_latent_steps=self.config.n_latent_steps,
            n_deep_recursions=self.config.n_deep_recursions,
            dropout=self.config.dropout,
        )
        self.trm.to(self.device)

        num_params = sum(p.numel() for p in self.trm.parameters())
        print(f"TRM initialized. Parameters: {num_params:,}")

    def _init_optimizer(self) -> None:
        """Initialize optimizer and scheduler."""
        self.optimizer = AdamW(
            self.trm.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            betas=(0.9, 0.95),  # TRM paper uses these betas
        )

    def _init_ema(self) -> None:
        """Initialize EMA if enabled."""
        self.ema: EMA | None = None
        if self.config.use_ema:
            self.ema = EMA(self.trm, decay=self.config.ema_decay)
            print(f"EMA enabled with decay={self.config.ema_decay}")

    def _init_wandb(self) -> None:
        """Initialize wandb logging."""
        if not self.config.use_wandb or not WANDB_AVAILABLE:
            return

        run_name = self.config.wandb_run_name or f"phase2_seq_d{self.config.d_compressed}"
        wandb.init(
            project=self.config.wandb_project,
            name=run_name,
            config={
                "d_compressed": self.config.d_compressed,
                "n_layers": self.config.n_layers,
                "n_latent_steps": self.config.n_latent_steps,
                "n_deep_recursions": self.config.n_deep_recursions,
                "n_supervision_steps": self.config.n_supervision_steps,
                "batch_size": self.config.batch_size,
                "learning_rate": self.config.learning_rate,
                "use_ema": self.config.use_ema,
            },
        )

    def _load_data(self) -> DataLoader:
        """Load hidden state sequences from phase2_datagen."""
        print(f"Loading data from {self.config.data_path}...")
        data = torch.load(self.config.data_path, map_location="cpu", weights_only=False)

        hidden_pre_list = data["hidden_pre"]  # List of [L_i, D]
        hidden_post = data["hidden_post"]  # [N, D]

        print(f"Loaded {len(hidden_pre_list)} samples")
        print(f"Hidden size: {data.get('hidden_size', hidden_post.shape[-1])}")
        print(f"Avg context length: {sum(data['seq_lengths']) / len(data['seq_lengths']):.1f}")

        # Create dataset (pre-compresses sequences)
        dataset = HiddenStateSequenceDataset(
            hidden_pre_list=hidden_pre_list,
            hidden_post=hidden_post,
            compressor=self.compressor,
            device=self.device,
        )

        dataloader = DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            collate_fn=collate_variable_length,
            num_workers=0,  # Pre-compressed data, no need for workers
            pin_memory=False,
        )

        return dataloader

    def _compute_metrics(self, pred: torch.Tensor, target: torch.Tensor) -> dict[str, float]:
        """Compute training metrics."""
        with torch.no_grad():
            mse = F.mse_loss(pred, target).item()
            cosine_sim = F.cosine_similarity(pred, target, dim=-1).mean().item()
            rel_error = (torch.norm(pred - target) / torch.norm(target)).item()

        return {
            "mse": mse,
            "cosine_similarity": cosine_sim,
            "relative_error": rel_error,
        }

    def _train_step(
        self,
        contexts: torch.Tensor,
        targets: torch.Tensor,
        mask: torch.Tensor,
    ) -> tuple[torch.Tensor, dict[str, float]]:
        """
        Single training step with optional halting loss.

        Args:
            contexts: Padded compressed contexts [B, L, D']
            targets: Compressed targets [B, D']
            mask: Attention mask [B, L]
        """
        if self.config.use_halting:
            # Run with halting to get per-step halt probabilities
            output, halt_probs, steps_taken = self.trm.forward_with_halting(
                contexts,
                mask=mask,
                n_steps=self.config.n_supervision_steps,
                early_stop=self.config.early_stop_training,
            )

            # Extract reasoning token (last position)
            reasoning_output = output[:, -1, :]  # [B, D']

            # MSE loss on reasoning token vs target
            mse_loss = F.mse_loss(reasoning_output, targets)

            # Halting loss: train halt_head to predict when output is "correct"
            # "Correct" = cosine_similarity > threshold
            halt_loss = torch.tensor(0.0, device=contexts.device)
            for halt_prob in halt_probs:
                # Compute similarity at this step
                cos_sim = F.cosine_similarity(reasoning_output, targets, dim=-1)  # [B]
                # Target: halt if similarity exceeds threshold
                halt_target = (cos_sim > self.config.halt_threshold).float().unsqueeze(-1)  # [B, 1]
                # BCE loss for halting
                halt_loss = halt_loss + F.binary_cross_entropy(
                    halt_prob.clamp(1e-7, 1 - 1e-7), halt_target
                )

            # Combine losses
            loss = mse_loss + self.config.halt_loss_weight * halt_loss

            # Compute metrics
            metrics = self._compute_metrics(reasoning_output, targets)
            metrics["halt_loss"] = halt_loss.item()
            metrics["steps_taken"] = steps_taken
            metrics["avg_halt_prob"] = torch.stack(halt_probs).mean().item()

        else:
            # Original behavior without halting
            output = self.trm(contexts, mask=mask, n_steps=self.config.n_supervision_steps)
            reasoning_output = output[:, -1, :]
            loss = F.mse_loss(reasoning_output, targets)
            metrics = self._compute_metrics(reasoning_output, targets)

        return loss, metrics

    def _save_checkpoint(self, epoch: int, loss: float, is_best: bool = False) -> None:
        """Save checkpoint."""
        checkpoint = {
            "epoch": epoch,
            "trm_state_dict": self.trm.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "loss": loss,
            "config": self.config,
        }

        if self.ema is not None:
            checkpoint["ema_shadow"] = self.ema.shadow

        if is_best:
            path = os.path.join(self.config.output_dir, "best.pt")
            torch.save(checkpoint, path)
            print(f"Saved best checkpoint (loss={loss:.6f})")

        # Also save periodic checkpoint
        path = os.path.join(self.config.output_dir, f"checkpoint_epoch{epoch}.pt")
        torch.save(checkpoint, path)

    def train(self) -> None:
        """Main training loop."""
        dataloader = self._load_data()

        # Create scheduler
        num_training_steps = len(dataloader) * self.config.num_epochs
        num_warmup_steps = int(num_training_steps * self.config.warmup_ratio)
        scheduler = CosineAnnealingLR(self.optimizer, T_max=num_training_steps - num_warmup_steps)

        self.trm.train()
        global_step = 0

        print("\nStarting Phase 2 training...")
        print(f"  Epochs: {self.config.num_epochs}")
        print(f"  Batch size: {self.config.batch_size}")
        print(f"  Steps per epoch: {len(dataloader)}")
        print(f"  Total steps: {num_training_steps}")

        for epoch in range(self.config.num_epochs):
            epoch_loss = 0.0
            epoch_metrics: dict[str, float] = {
                "mse": 0.0,
                "cosine_similarity": 0.0,
                "relative_error": 0.0,
                "halt_loss": 0.0,
                "steps_taken": 0.0,
                "avg_halt_prob": 0.0,
            }
            num_batches = 0

            pbar = tqdm(dataloader, desc=f"Epoch {epoch + 1}/{self.config.num_epochs}")

            for contexts, targets, mask in pbar:
                contexts = contexts.to(self.device)
                targets = targets.to(self.device)
                mask = mask.to(self.device)

                # Forward and loss
                loss, metrics = self._train_step(contexts, targets, mask)

                # NaN detection
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"\nWarning: NaN/Inf loss detected at step {global_step}")
                    print(f"  Context stats: min={contexts.min():.4f}, max={contexts.max():.4f}")
                    print(f"  Target stats: min={targets.min():.4f}, max={targets.max():.4f}")
                    # Skip this batch
                    continue

                # Backward
                self.optimizer.zero_grad()
                loss.backward()

                # Clip gradients
                torch.nn.utils.clip_grad_norm_(self.trm.parameters(), self.config.max_grad_norm)

                self.optimizer.step()
                scheduler.step()

                # Update EMA
                if self.ema is not None:
                    self.ema.update()

                # Track metrics
                epoch_loss += loss.item()
                for k, v in metrics.items():
                    epoch_metrics[k] += v
                num_batches += 1
                global_step += 1

                # Logging
                if global_step % self.config.log_steps == 0:
                    avg_loss = epoch_loss / num_batches
                    avg_cos_sim = epoch_metrics["cosine_similarity"] / num_batches
                    pbar.set_postfix({"loss": f"{avg_loss:.6f}", "cos_sim": f"{avg_cos_sim:.4f}"})

                    if self.config.use_wandb and WANDB_AVAILABLE:
                        log_dict = {
                            "train/loss": avg_loss,
                            "train/mse": epoch_metrics["mse"] / num_batches,
                            "train/cosine_similarity": avg_cos_sim,
                            "train/relative_error": epoch_metrics["relative_error"]
                            / num_batches,
                            "train/lr": scheduler.get_last_lr()[0],
                        }
                        if self.config.use_halting:
                            log_dict["train/halt_loss"] = epoch_metrics["halt_loss"] / num_batches
                            log_dict["train/steps_taken"] = epoch_metrics["steps_taken"] / num_batches
                            log_dict["train/avg_halt_prob"] = epoch_metrics["avg_halt_prob"] / num_batches
                        wandb.log(log_dict, step=global_step)

            # End of epoch
            avg_epoch_loss = epoch_loss / num_batches
            avg_epoch_metrics = {k: v / num_batches for k, v in epoch_metrics.items()}

            print(
                f"Epoch {epoch + 1} complete. "
                f"Loss: {avg_epoch_loss:.6f}, "
                f"Cosine Sim: {avg_epoch_metrics['cosine_similarity']:.4f}"
            )

            # Save best checkpoint
            if avg_epoch_loss < self.best_loss:
                self.best_loss = avg_epoch_loss
                self._save_checkpoint(epoch + 1, avg_epoch_loss, is_best=True)

            # Log epoch metrics
            if self.config.use_wandb and WANDB_AVAILABLE:
                wandb.log(
                    {
                        "epoch/loss": avg_epoch_loss,
                        "epoch/cosine_similarity": avg_epoch_metrics["cosine_similarity"],
                        "epoch/best_loss": self.best_loss,
                    },
                    step=global_step,
                )

        print("\nTraining complete!")
        print(f"Best loss: {self.best_loss:.6f}")

        if self.config.use_wandb and WANDB_AVAILABLE:
            wandb.finish()


def run_phase2_training(config: Phase2Config | None = None) -> None:
    """Main entry point for Phase 2 training."""
    config = config or Phase2Config()
    trainer = TRMSequenceTrainer(config)
    trainer.train()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Phase 2: TRM Sequence Training")
    parser.add_argument(
        "--data_path",
        type=str,
        default="./data/hidden_pairs/hidden_pairs.pt",
        help="Path to hidden state pairs from phase2_datagen",
    )
    parser.add_argument(
        "--compressor_checkpoint",
        type=str,
        default="anonx3247/llm-trm-compressor",
        help="HF Hub repo or local path to compressor checkpoint",
    )
    parser.add_argument("--output_dir", type=str, default="./checkpoints/phase2")
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--n_latent_steps", type=int, default=6, help="Latent recursion steps (n)")
    parser.add_argument("--n_deep_recursions", type=int, default=3, help="Deep recursion steps (T)")
    parser.add_argument(
        "--n_supervision_steps", type=int, default=8, help="Supervision steps (N_sup)"
    )
    parser.add_argument("--dropout", type=float, default=0.1, help="Dropout for stability")

    # Halting (ACT)
    parser.add_argument("--use_halting", action="store_true", default=True, help="Train halt head")
    parser.add_argument("--no_halting", action="store_false", dest="use_halting")
    parser.add_argument(
        "--halt_threshold", type=float, default=0.95, help="Cosine sim threshold for halt target"
    )
    parser.add_argument(
        "--halt_loss_weight", type=float, default=0.5, help="Weight for halting loss"
    )
    parser.add_argument(
        "--early_stop_training",
        action="store_true",
        default=False,
        help="Early stop during training when halt_prob > 0.5",
    )

    # EMA
    parser.add_argument("--use_ema", action="store_true", default=True)
    parser.add_argument("--no_ema", action="store_false", dest="use_ema")
    parser.add_argument("--ema_decay", type=float, default=0.999)

    # Wandb
    parser.add_argument("--use_wandb", action="store_true", default=True)
    parser.add_argument("--no_wandb", action="store_false", dest="use_wandb")
    parser.add_argument("--wandb_project", type=str, default="llm-trm-phase2")
    parser.add_argument("--wandb_run_name", type=str, default=None)

    parser.add_argument("--seed", type=int, default=42)

    args = parser.parse_args()

    config = Phase2Config(
        data_path=args.data_path,
        compressor_checkpoint=args.compressor_checkpoint,
        output_dir=args.output_dir,
        batch_size=args.batch_size,
        num_epochs=args.num_epochs,
        learning_rate=args.learning_rate,
        n_latent_steps=args.n_latent_steps,
        n_deep_recursions=args.n_deep_recursions,
        n_supervision_steps=args.n_supervision_steps,
        dropout=args.dropout,
        use_halting=args.use_halting,
        halt_threshold=args.halt_threshold,
        halt_loss_weight=args.halt_loss_weight,
        early_stop_training=args.early_stop_training,
        use_ema=args.use_ema,
        ema_decay=args.ema_decay,
        use_wandb=args.use_wandb,
        wandb_project=args.wandb_project,
        wandb_run_name=args.wandb_run_name,
        seed=args.seed,
    )

    run_phase2_training(config)
