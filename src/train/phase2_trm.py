"""
Phase 2: TRM Iteration Training

Train the TRM to replace LLM's chain-of-thought with latent iterations.

Goal: TRM learns to map hidden_pre_<thinking> -> hidden_post_</thinking>

Training follows the TRM paper methodology:
1. Input: Compressed hidden_pre states
2. Target: Compressed hidden_post states
3. TRM iterates to reach target:
   - Latent recursion: z = net(x + y + z) repeated n times
   - Prediction update: y = net(y + z)
   - Deep recursion: T-1 without gradients, 1 with
4. Loss: ||TRM(compress(hidden_pre)) - compress(hidden_post)||^2
5. ACT mechanism to learn iteration count

Uses data generated by phase2_datagen.py

Usage:
    python -m src.train.phase2_trm \\
        --data_path ./data/hidden_pairs \\
        --output_dir ./checkpoints/phase2 \\
        --num_epochs 100

TODO: Implement training logic following TRM paper
"""

from dataclasses import dataclass

import torch


@dataclass
class Phase2Config:
    """Configuration for Phase 2 TRM training"""

    # Data
    data_path: str = "./data/hidden_pairs"

    # Model (from TRM paper)
    hidden_size: int = 3072  # SmolLM3 hidden size
    num_latents: int = 256
    n_layers: int = 2  # TRM paper: 2 layers is optimal
    n_heads: int = 8
    n_latent_steps: int = 6  # n in paper
    n_deep_recursions: int = 3  # T in paper
    n_supervision_steps: int = 16  # N_sup in paper

    # Training (from TRM paper hyperparameters)
    batch_size: int = 768  # Paper uses 768
    learning_rate: float = 1e-4
    embedding_lr: float = 1e-2  # Higher LR for embeddings
    weight_decay: float = 0.1  # 0.1 for reasoning tasks
    num_epochs: int = 100
    warmup_steps: int = 2000

    # EMA (critical for stability per paper)
    use_ema: bool = True
    ema_decay: float = 0.999

    # ACT (Adaptive Computational Time)
    use_act: bool = True

    # Output
    output_dir: str = "./checkpoints/phase2"
    checkpoint_interval: int = 1000


class TRMIterationTrainer:
    """
    Phase 2 trainer: TRM learns to replicate CoT in latent space.

    Training loop (from TRM paper Algorithm 2):
    ```
    for x_input, y_true in train_dataloader:
        y, z = y_init, z_init
        for step in range(N_supervision):
            x = input_embedding(x_input)
            (y, z), y_hat, q_hat = deep_recursion(x, y, z)
            loss = softmax_cross_entropy(y_hat, y_true)
            loss += binary_cross_entropy(q_hat, (y_hat == y_true))
            loss.backward()
            opt.step()
            opt.zero_grad()
            if q_hat > 0:  # early-stopping
                break
    ```

    Key differences from paper:
    - Input x = compress(hidden_pre) instead of embedded tokens
    - Target y_true = compress(hidden_post) instead of tokens
    - Loss = MSE on hidden states instead of cross-entropy

    TODO: Implement training loop
    """

    def __init__(self, config: Phase2Config):
        self.config = config
        # TODO: Initialize TRM, compressor, optimizer, EMA, etc.

    def _init_ema(self) -> None:
        """
        Initialize Exponential Moving Average of model weights.

        From TRM paper: EMA=0.999 is critical for training stability
        on small datasets.

        TODO: Implement
        """
        raise NotImplementedError("EMA not implemented")

    def _compute_loss(
        self, y_pred: torch.Tensor, y_true: torch.Tensor, halt_prob: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute training loss.

        Components:
        1. Reconstruction loss: MSE(y_pred, y_true)
        2. ACT loss: BCE(halt_prob, reached_target)

        TODO: Implement
        """
        raise NotImplementedError("Loss computation not implemented")

    def _deep_supervision_step(
        self, hidden_pre: torch.Tensor, hidden_post: torch.Tensor
    ) -> torch.Tensor:
        """
        Single deep supervision training step.

        Following TRM paper:
        1. Compress inputs: x = compress(hidden_pre)
        2. Initialize y, z = zeros
        3. For each supervision step:
           a. Run deep recursion (T-1 no grad, 1 with grad)
           b. Compute loss against compress(hidden_post)
           c. Backprop
           d. Check ACT halting
        4. Return total loss

        TODO: Implement
        """
        raise NotImplementedError("Deep supervision step not implemented")

    def train_epoch(self) -> float:
        """
        Train for one epoch.

        TODO: Implement
        """
        raise NotImplementedError("Training epoch not implemented")

    def train(self) -> None:
        """
        Main training loop.

        TODO: Implement
        """
        raise NotImplementedError(
            "Phase 2 (TRM iteration training) not yet implemented.\n"
            "See papers/less-is-more-TRM/paper.tex for:\n"
            "- Algorithm 2 (TRM training loop)\n"
            "- Hyperparameters section\n"
            "- Deep supervision and ACT details"
        )


def run_phase2_training(config: Phase2Config | None = None) -> None:
    """
    Main entry point for Phase 2 training.

    TODO: Implement
    """
    config = config or Phase2Config()
    trainer = TRMIterationTrainer(config)
    trainer.train()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Phase 2: TRM Iteration Training")
    parser.add_argument("--data_path", type=str, default="./data/hidden_pairs")
    parser.add_argument("--output_dir", type=str, default="./checkpoints/phase2")
    parser.add_argument("--batch_size", type=int, default=768)
    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--n_latent_steps", type=int, default=6)
    parser.add_argument("--n_deep_recursions", type=int, default=3)
    args = parser.parse_args()

    config = Phase2Config(
        data_path=args.data_path,
        output_dir=args.output_dir,
        batch_size=args.batch_size,
        num_epochs=args.num_epochs,
        learning_rate=args.learning_rate,
        n_latent_steps=args.n_latent_steps,
        n_deep_recursions=args.n_deep_recursions,
    )

    run_phase2_training(config)
