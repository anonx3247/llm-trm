"""
Phase 2: TRM Iteration Training

Train the TRM to replace LLM's chain-of-thought with latent iterations.

Goal: TRM learns to map hidden_pre_<thinking> -> hidden_post_</thinking>

Training follows the TRM paper methodology:
1. Input: Compressed hidden_pre states
2. Target: Compressed hidden_post states
3. TRM iterates to reach target:
   - Latent recursion: z = net(x + y + z) repeated n times
   - Prediction update: y = net(y + z)
   - Deep recursion: T-1 without gradients, 1 with
4. Loss: ||TRM(compress(hidden_pre)) - compress(hidden_post)||^2

Uses data generated by phase2_datagen.py

Usage:
    python -m src.train.phase2_trm \
        --data_path ./data/hidden_pairs/hidden_pairs.pt \
        --compressor_checkpoint anonx3247/llm-trm-compressor \
        --output_dir ./checkpoints/phase2 \
        --num_epochs 100
"""

import os
import sys
from dataclasses import dataclass
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
from huggingface_hub import hf_hub_download
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm

from src.models.compression import DimensionCompressor
from src.models.trm import RecursiveReasoningBase, TinyRecursiveNetwork
from src.train.phase1_compressor import Phase1Config

# Optional wandb import
try:
    import wandb

    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    wandb = None  # type: ignore[assignment]


@dataclass
class Phase2Config:
    """Configuration for Phase 2 TRM training"""

    # Data
    data_path: str = "./data/hidden_pairs/hidden_pairs.pt"

    # Compressor (from Phase 1)
    compressor_checkpoint: str = "anonx3247/llm-trm-compressor"

    # Model (from TRM paper)
    hidden_size: int = 2048  # SmolLM3 actual hidden size
    d_compressed: int = 256  # Must match compressor
    n_layers: int = 2  # TRM paper: 2 layers is optimal
    n_heads: int = 8
    n_latent_steps: int = 6  # n in paper
    n_deep_recursions: int = 3  # T in paper
    n_supervision_steps: int = 16  # N_sup in paper

    # Training (from TRM paper hyperparameters)
    batch_size: int = 64  # Smaller than paper's 768 for hidden states
    learning_rate: float = 1e-4
    weight_decay: float = 0.1  # 0.1 for reasoning tasks
    num_epochs: int = 100
    warmup_ratio: float = 0.1
    max_grad_norm: float = 1.0
    gradient_accumulation_steps: int = 1

    # EMA (critical for stability per paper)
    use_ema: bool = True
    ema_decay: float = 0.999

    # Output
    output_dir: str = "./checkpoints/phase2"
    log_steps: int = 10
    save_steps: int = 1000

    # Wandb
    use_wandb: bool = True
    wandb_project: str = "llm-trm-phase2"
    wandb_run_name: str | None = None

    # Reproducibility
    seed: int = 42


class HiddenStateTRMForTraining(RecursiveReasoningBase):
    """
    TRM for Phase 2 training on hidden state pairs.

    Differs from HiddenStateTRM in smollm.py:
    - Takes a pre-trained compressor (frozen)
    - Designed for training with MSE loss
    - No halting during training
    """

    def __init__(
        self,
        d_compressed: int = 256,
        n_layers: int = 2,
        n_heads: int = 8,
        n_latent_steps: int = 6,
        n_deep_recursions: int = 3,
        dropout: float = 0.1,
    ):
        super().__init__()

        self.d_compressed = d_compressed
        self.n_latent_steps = n_latent_steps
        self.n_deep_recursions = n_deep_recursions

        # TRM network operates in compressed dimension space
        self.net = TinyRecursiveNetwork(
            d_model=d_compressed, n_layers=n_layers, n_heads=n_heads, dropout=dropout
        )

        # Halting mechanism (required by base class)
        self.halt_head = nn.Linear(d_compressed, 1)

    def forward(self, x: torch.Tensor, n_steps: int = 1) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Run TRM reasoning on compressed hidden states.

        Args:
            x: Compressed input [B, D'] or [B, 1, D']
            n_steps: Number of supervision steps

        Returns:
            y: Final prediction [B, D'] or [B, 1, D']
            z: Final latent state (for inspection)
        """
        # Handle both [B, D'] and [B, 1, D'] inputs
        squeeze_output = False
        if x.dim() == 2:
            x = x.unsqueeze(1)  # [B, D'] -> [B, 1, D']
            squeeze_output = True

        # Initialize y and z
        y = torch.zeros_like(x)
        z = torch.zeros_like(x)

        # Run supervision steps
        for _ in range(n_steps):
            y, z = self.run_deep_recursion(x, y, z, with_gradients=True)

        if squeeze_output:
            y = y.squeeze(1)  # [B, 1, D'] -> [B, D']
            z = z.squeeze(1)

        return y, z


class EMA:
    """Exponential Moving Average for model weights."""

    def __init__(self, model: nn.Module, decay: float = 0.999):
        self.model = model
        self.decay = decay
        self.shadow: dict[str, torch.Tensor] = {}
        self.backup: dict[str, torch.Tensor] = {}

        # Initialize shadow weights
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()

    def update(self) -> None:
        """Update shadow weights with current model weights."""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = self.decay * self.shadow[name] + (1 - self.decay) * param.data

    def apply_shadow(self) -> None:
        """Apply shadow weights to model (for evaluation)."""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data.clone()
                param.data = self.shadow[name]

    def restore(self) -> None:
        """Restore original weights after evaluation."""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]
        self.backup = {}


class TRMIterationTrainer:
    """
    Phase 2 trainer: TRM learns to map hidden_pre -> hidden_post in compressed space.

    Training loop:
    1. Load pre-trained compressor (frozen)
    2. Compress hidden_pre and hidden_post
    3. TRM iterates on compressed hidden_pre to reach compressed hidden_post
    4. Loss: MSE between TRM output and compressed hidden_post
    """

    def __init__(self, config: Phase2Config):
        self.config = config
        self.device = self._get_device()
        self.best_loss = float("inf")

        # Create output directory
        Path(config.output_dir).mkdir(parents=True, exist_ok=True)

        # Initialize components
        self._load_compressor()
        self._init_trm()
        self._init_optimizer()
        self._init_ema()
        self._init_wandb()

    def _get_device(self) -> torch.device:
        """Determine device to use."""
        if torch.cuda.is_available():
            return torch.device("cuda")
        elif torch.backends.mps.is_available():
            return torch.device("mps")
        return torch.device("cpu")

    def _load_compressor(self) -> None:
        """Load pre-trained compressor from Phase 1."""
        ckpt_source = self.config.compressor_checkpoint

        # Download from HF Hub if needed
        if "/" in ckpt_source and not os.path.exists(ckpt_source):
            print(f"Downloading compressor from HF Hub: {ckpt_source}...")
            ckpt_path = hf_hub_download(repo_id=ckpt_source, filename="compressor.pt")
        else:
            ckpt_path = ckpt_source

        # Fix pickle compatibility
        sys.modules["__main__"].Phase1Config = Phase1Config  # type: ignore[attr-defined]
        checkpoint = torch.load(ckpt_path, map_location=self.device, weights_only=False)

        # Get config from checkpoint
        ckpt_config = checkpoint.get("config")
        if ckpt_config:
            self.config.hidden_size = ckpt_config.hidden_size
            self.config.d_compressed = ckpt_config.d_compressed
            print(
                f"Loaded compressor config: hidden_size={self.config.hidden_size}, d_compressed={self.config.d_compressed}"
            )

        # Initialize compressor
        self.compressor = DimensionCompressor(
            d_model=self.config.hidden_size,
            d_compressed=self.config.d_compressed,
        )

        # Load state dict (handle torch.compile prefix)
        state_dict = checkpoint["compressor"]
        state_dict = {k.replace("_orig_mod.", ""): v for k, v in state_dict.items()}
        self.compressor.load_state_dict(state_dict)
        self.compressor.to(self.device)
        self.compressor.eval()

        # Freeze compressor
        for param in self.compressor.parameters():
            param.requires_grad = False

        print(
            f"Compressor loaded and frozen. Compression ratio: {self.config.hidden_size / self.config.d_compressed:.1f}x"
        )

    def _init_trm(self) -> None:
        """Initialize TRM model."""
        self.trm = HiddenStateTRMForTraining(
            d_compressed=self.config.d_compressed,
            n_layers=self.config.n_layers,
            n_heads=self.config.n_heads,
            n_latent_steps=self.config.n_latent_steps,
            n_deep_recursions=self.config.n_deep_recursions,
        )
        self.trm.to(self.device)

        num_params = sum(p.numel() for p in self.trm.parameters())
        print(f"TRM initialized. Parameters: {num_params:,}")

    def _init_optimizer(self) -> None:
        """Initialize optimizer and scheduler."""
        self.optimizer = AdamW(
            self.trm.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            betas=(0.9, 0.95),  # TRM paper uses these betas
        )

    def _init_ema(self) -> None:
        """Initialize EMA if enabled."""
        self.ema: EMA | None = None
        if self.config.use_ema:
            self.ema = EMA(self.trm, decay=self.config.ema_decay)
            print(f"EMA enabled with decay={self.config.ema_decay}")

    def _init_wandb(self) -> None:
        """Initialize wandb logging."""
        if not self.config.use_wandb or not WANDB_AVAILABLE:
            return

        run_name = self.config.wandb_run_name or f"phase2_d{self.config.d_compressed}"
        wandb.init(
            project=self.config.wandb_project,
            name=run_name,
            config={
                "d_compressed": self.config.d_compressed,
                "n_layers": self.config.n_layers,
                "n_latent_steps": self.config.n_latent_steps,
                "n_deep_recursions": self.config.n_deep_recursions,
                "n_supervision_steps": self.config.n_supervision_steps,
                "batch_size": self.config.batch_size,
                "learning_rate": self.config.learning_rate,
                "use_ema": self.config.use_ema,
            },
        )

    def _load_data(self) -> DataLoader:
        """Load hidden state pairs from phase2_datagen."""
        print(f"Loading data from {self.config.data_path}...")
        data = torch.load(self.config.data_path, map_location="cpu", weights_only=True)

        hidden_pre = data["hidden_pre"]  # [N, D]
        hidden_post = data["hidden_post"]  # [N, D]

        print(f"Loaded {len(hidden_pre)} samples")
        print(f"Hidden state shape: {hidden_pre.shape}")

        # Create dataset
        dataset = TensorDataset(hidden_pre, hidden_post)
        dataloader = DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=4,
            pin_memory=True,
        )

        return dataloader

    def _compute_metrics(self, pred: torch.Tensor, target: torch.Tensor) -> dict[str, float]:
        """Compute training metrics."""
        with torch.no_grad():
            mse = F.mse_loss(pred, target).item()
            cosine_sim = F.cosine_similarity(pred, target, dim=-1).mean().item()
            rel_error = (torch.norm(pred - target) / torch.norm(target)).item()

        return {
            "mse": mse,
            "cosine_similarity": cosine_sim,
            "relative_error": rel_error,
        }

    def _train_step(
        self, hidden_pre: torch.Tensor, hidden_post: torch.Tensor
    ) -> tuple[torch.Tensor, dict[str, float]]:
        """Single training step."""
        # Compress inputs (compressor is frozen)
        with torch.no_grad():
            x = self.compressor(hidden_pre)  # [B, D']
            target = self.compressor(hidden_post)  # [B, D']

        # TRM forward
        y, _ = self.trm(x, n_steps=self.config.n_supervision_steps)

        # MSE loss
        loss = F.mse_loss(y, target)

        # Compute metrics
        metrics = self._compute_metrics(y, target)

        return loss, metrics

    def _save_checkpoint(self, epoch: int, loss: float, is_best: bool = False) -> None:
        """Save checkpoint."""
        checkpoint = {
            "epoch": epoch,
            "trm_state_dict": self.trm.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "loss": loss,
            "config": self.config,
        }

        if self.ema is not None:
            checkpoint["ema_shadow"] = self.ema.shadow

        if is_best:
            path = os.path.join(self.config.output_dir, "best.pt")
            torch.save(checkpoint, path)
            print(f"Saved best checkpoint (loss={loss:.6f})")

        # Also save periodic checkpoint
        path = os.path.join(self.config.output_dir, f"checkpoint_epoch{epoch}.pt")
        torch.save(checkpoint, path)

    def train(self) -> None:
        """Main training loop."""
        dataloader = self._load_data()

        # Create scheduler
        num_training_steps = len(dataloader) * self.config.num_epochs
        num_warmup_steps = int(num_training_steps * self.config.warmup_ratio)
        scheduler = CosineAnnealingLR(self.optimizer, T_max=num_training_steps - num_warmup_steps)

        self.trm.train()
        global_step = 0

        print("\nStarting Phase 2 training...")
        print(f"  Epochs: {self.config.num_epochs}")
        print(f"  Batch size: {self.config.batch_size}")
        print(f"  Steps per epoch: {len(dataloader)}")
        print(f"  Total steps: {num_training_steps}")

        for epoch in range(self.config.num_epochs):
            epoch_loss = 0.0
            epoch_metrics: dict[str, float] = {
                "mse": 0.0,
                "cosine_similarity": 0.0,
                "relative_error": 0.0,
            }
            num_batches = 0

            pbar = tqdm(dataloader, desc=f"Epoch {epoch + 1}/{self.config.num_epochs}")

            for hidden_pre, hidden_post in pbar:
                hidden_pre = hidden_pre.to(self.device)
                hidden_post = hidden_post.to(self.device)

                # Forward and loss
                loss, metrics = self._train_step(hidden_pre, hidden_post)

                # Backward
                self.optimizer.zero_grad()
                loss.backward()

                # Clip gradients
                torch.nn.utils.clip_grad_norm_(self.trm.parameters(), self.config.max_grad_norm)

                self.optimizer.step()
                scheduler.step()

                # Update EMA
                if self.ema is not None:
                    self.ema.update()

                # Track metrics
                epoch_loss += loss.item()
                for k, v in metrics.items():
                    epoch_metrics[k] += v
                num_batches += 1
                global_step += 1

                # Logging
                if global_step % self.config.log_steps == 0:
                    avg_loss = epoch_loss / num_batches
                    avg_cos_sim = epoch_metrics["cosine_similarity"] / num_batches
                    pbar.set_postfix({"loss": f"{avg_loss:.6f}", "cos_sim": f"{avg_cos_sim:.4f}"})

                    if self.config.use_wandb and WANDB_AVAILABLE:
                        wandb.log(
                            {
                                "train/loss": avg_loss,
                                "train/mse": epoch_metrics["mse"] / num_batches,
                                "train/cosine_similarity": avg_cos_sim,
                                "train/relative_error": epoch_metrics["relative_error"]
                                / num_batches,
                                "train/lr": scheduler.get_last_lr()[0],
                            },
                            step=global_step,
                        )

            # End of epoch
            avg_epoch_loss = epoch_loss / num_batches
            avg_epoch_metrics = {k: v / num_batches for k, v in epoch_metrics.items()}

            print(
                f"Epoch {epoch + 1} complete. "
                f"Loss: {avg_epoch_loss:.6f}, "
                f"Cosine Sim: {avg_epoch_metrics['cosine_similarity']:.4f}"
            )

            # Save best checkpoint
            if avg_epoch_loss < self.best_loss:
                self.best_loss = avg_epoch_loss
                self._save_checkpoint(epoch + 1, avg_epoch_loss, is_best=True)

            # Log epoch metrics
            if self.config.use_wandb and WANDB_AVAILABLE:
                wandb.log(
                    {
                        "epoch/loss": avg_epoch_loss,
                        "epoch/cosine_similarity": avg_epoch_metrics["cosine_similarity"],
                        "epoch/best_loss": self.best_loss,
                    },
                    step=global_step,
                )

        print("\nTraining complete!")
        print(f"Best loss: {self.best_loss:.6f}")

        if self.config.use_wandb and WANDB_AVAILABLE:
            wandb.finish()


def run_phase2_training(config: Phase2Config | None = None) -> None:
    """Main entry point for Phase 2 training."""
    config = config or Phase2Config()
    trainer = TRMIterationTrainer(config)
    trainer.train()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Phase 2: TRM Iteration Training")
    parser.add_argument(
        "--data_path",
        type=str,
        default="./data/hidden_pairs/hidden_pairs.pt",
        help="Path to hidden state pairs from phase2_datagen",
    )
    parser.add_argument(
        "--compressor_checkpoint",
        type=str,
        default="anonx3247/llm-trm-compressor",
        help="HF Hub repo or local path to compressor checkpoint",
    )
    parser.add_argument("--output_dir", type=str, default="./checkpoints/phase2")
    parser.add_argument("--batch_size", type=int, default=64)
    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--n_latent_steps", type=int, default=6)
    parser.add_argument("--n_deep_recursions", type=int, default=3)
    parser.add_argument("--n_supervision_steps", type=int, default=16)

    # EMA
    parser.add_argument("--use_ema", action="store_true", default=True)
    parser.add_argument("--no_ema", action="store_false", dest="use_ema")
    parser.add_argument("--ema_decay", type=float, default=0.999)

    # Wandb
    parser.add_argument("--use_wandb", action="store_true", default=True)
    parser.add_argument("--no_wandb", action="store_false", dest="use_wandb")
    parser.add_argument("--wandb_project", type=str, default="llm-trm-phase2")
    parser.add_argument("--wandb_run_name", type=str, default=None)

    parser.add_argument("--seed", type=int, default=42)

    args = parser.parse_args()

    config = Phase2Config(
        data_path=args.data_path,
        compressor_checkpoint=args.compressor_checkpoint,
        output_dir=args.output_dir,
        batch_size=args.batch_size,
        num_epochs=args.num_epochs,
        learning_rate=args.learning_rate,
        n_latent_steps=args.n_latent_steps,
        n_deep_recursions=args.n_deep_recursions,
        n_supervision_steps=args.n_supervision_steps,
        use_ema=args.use_ema,
        ema_decay=args.ema_decay,
        use_wandb=args.use_wandb,
        wandb_project=args.wandb_project,
        wandb_run_name=args.wandb_run_name,
        seed=args.seed,
    )

    run_phase2_training(config)
